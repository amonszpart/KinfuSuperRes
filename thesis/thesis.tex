\documentclass{ucl_thesis}

%twoside for double page printing
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{color}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage{algorithmic}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[square]{natbib}

\usepackage[font=footnotesize,labelfont=bf,singlelinecheck=on]{caption}
\usepackage[hyphens]{url}

\usepackage{amsmath}
% Uncomment this to activate the TikZ library, which is useful for fancy block-diagrams.
%    \usepackage{tikz}
%    \usetikzlibrary{positioning,arrows,shapes.misc}
\usepackage{multirow}

%\numberwithin{algorithm}{chapter}
%\usepackage{epsf}
\usepackage{fancyvrb}

%\documentclass[12pt,a4paper]{article}
%\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
%\usepackage{amsmath}
%\usepackage{amsfonts}
%\usepackage{amssymb}
%\usepackage{graphicx}
%usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
%
%\usepackage[]{natbib}

% // TODO: add pagenumbers

%% LISTINGS
\usepackage{color}
\usepackage{listings}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\definecolor{purple}{rgb}{0.28,0.23,0.5}
\lstdefinestyle{customcpp}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  frame=L,
  xleftmargin=\parindent,
  language=C++,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\bfseries\color{green},
  commentstyle=\itshape\color{purple},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
  numbers        = left,
  stepnumber     = 5,
}
%%

%% figure template
\newcommand{\myfig}[6]{%
\begin{figure}[h!]\centering%
	\begin{minipage}[b]{0.49\linewidth}\centering%
		\includegraphics[width=\textwidth]{#2}%
		\caption{#3}%
		\label{fig:#1}%
	\end{minipage}%
	\begin{minipage}[b]{0.49\linewidth}\centering%
		\includegraphics[width=\textwidth]{#5}%
		\caption{#6}%
		\label{fig:#4}%
	\end{minipage}%
\end{figure}%
}
%%

%% shortcuts
%Malcolm added some custom commands here - you can make your own!
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\figref}[1]{(Fig. \ref{#1})}
\newcommand{\secref}[1]{(Section \ref{#1})}
\newcommand{\chpref}[1]{(Chapter \ref{#1})}
\def\etal{{et~al.}}
\def\ie{{\it i.e.,\ }}
\def\etc{{\it etc.,\ }}
\def\eg{{\it e.g.,\ }}
\def\vs{{\it vs.\ }}
\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
%%

\author{Aron Monszpart}
\title{Hand-held Scanning of Surface Details}
\def \supervisor {Dr. Gabriel Brostow}
\date{September 2013}

\begin{document}

\bibliographystyle{plainnat}
\maketitle
\numberwithin{algorithm}{chapter}
\setcounter{page}{1}
\pagenumbering{roman}
\pagestyle{plain}

% Notice how the star "*" is used throughout LaTex to modify the normal behavior. For example, \section* instead of \section 
% tells LaTex NOT to number this section.


\newpage
\section*{Abstract}
I've done some stuff
%\input{abstract.tex}


\newpage
\section*{Acknowledgements}
Dr. Gabriel Brostow, Dr Neill Campbell, Dr. Oisin Mac Aodha, Fabrizio Pece, Malcolm Reynolds, Clement Godard, Peter Rennert and the UCL PRISM group

%\setcounter{tocdepth}{2}
\tableofcontents
\listoffigures
%\listoftables
%\listofalgorithms
\newpage

\setcounter{page}{1}
\pagenumbering{arabic}
\pagestyle{plain}

\chapter{Introduction and Background} 

\par The targeted system attempts to solve the problem of 3D reconstruction of fine surface details in static scenes. We have seen various examples of sparse 3D reconstruction methods using the achievements of multi-view stereo, structure from motion and SLAM techniques in \secref{sec:SFM}. In recent years, a large leap has been made in dense 3D reconstruction by methods in \secref{sec:kinfu}. By designing this system we attempted to address inaccurracies produced by the above mentioned methods to refine high frequency details in small segments of the reconstructed models.

3D recon bevezetes
Depth sr bevezetes

\label{chp:background}
	\begin{itemize}	
		\item Motivation
		\item Overview of approach
		\begin{itemize}
			\item Calibration
			\item Low resolution 3D reconstruction
			\item Pose estimation of new input
			\item Mesh subdivision and 2D projection
			\item Depth upsampling
			\item Mesh enhancement by backprojection
		\end{itemize}
	\end{itemize}

\chapter{Related work} 
\label{chp:related_work}

\section{Multi view stereo}
\label{sec:mvs}
Spatioangular lightfields, disney: \citep{Kim:2013}

\section{Calibration}
\label{sec:lit_calib}

One of the initial steps of the setup of every multi-camera system has to be calibration. The expression calibration can be interpreted with several meanings. In this document we refer to calibration as the preprocessing step that captures the characteristics of the physical recording system accounting for, amongst others, inaccuracies in the physical manufacturing processes of the cameras. A successful calibration enables a correspondence to be calculated between data captured at the same time with the different cameras. This correspondence is data dependent in the case of depth cameras, and thus calculated every frame.\\

A wider interpretation of calibration includes the calculation of a nearly global position of the recording system. It usually is incorporated by the relative position estimation between the observed scene and the recording system, which can be viewed as global in case the global position of the observed scene is known. \\

Most previous works targeting to develop a highly accurate calibration system agree on the computational parameter model called pinhole-camera model \figref{eq:pinhole}. The model specifies the parameters: focal length ($f_x$, $f_y$), principal point ($c_x$, $c_y$) and skew ($\gamma$).

\begin{figure}[h!]
\begin{equation}
\lambda
\underbrace{
	\left[\begin{array}{ccc}
	    x \\
	    y \\
	    1 \\
	\end{array} \right]
}_{
	{\bf \tilde{x}}_{image}
}
%
=
%
\underbrace{
	\left[\begin{array}{cccc}
			f_x & \gamma & c_x & 0 \\
			0   & f_y    & c_y & 0 \\
	 		0   & 0      & 1   & 0 \\
	\end{array} \right]
}_{
	\left[\begin{array}{cc}
		\bf{\Lambda} & \bf{0}
	\end{array} \right]
}
%
\underbrace{
	\left[\begin{array}{cccc}
	    r_{1,1} & r_{1,2} & r_{1,3} & t_1 \\
	    r_{2,1} & r_{2,2} & r_{2,3} & t_2 \\
	    r_{3,1} & r_{3,2} & r_{3,3} & t_3 \\
	    0       & 0       & 0       & 1 \\
	\end{array} \right]
}_{
	\left[\begin{array}{cc}
		\bf{\Omega}     & \bf{\tau} \\
		\bf{0}^{\it{T}} 	& 1		      \\
	\end{array} \right]
}
%
\underbrace{
	\left[\begin{array}{c}
	    u \\
	    v \\
	    w \\
	    1 \\
	\end{array} \right]
}_{
	{\bf \tilde{x}}_{world}
}
\end{equation}
\caption{Correspondence between a point in global world coordinates (X,Y,Z) and camera image coordinates (u,v) according to the pinhole camera model. Source: UCL 2012 Machine Vision lecture notes. }
\label{eq:pinhole}
\end{figure}

Initial works \citep{Melen:1994}, \citep{Weng:1992} use the combination of linear, and non-linear minimisation techniques to calculate physically based camera parameters. In [9] non-physical implicit parameters are used in a two-step optimisation approach. The baseline calibration technique used in this project relies on the four-step calibration approach presented by \citep{Heikkila:1997}. They introduced a technique to estimate a four component set of distortion parameters in addition to the intrinsics of the pinhole camera. These parameters account for the radial ($k_1$, $k_2$) and tangential ($p_1$, $p_2$) distortion of the camera model. In parallel \citep{Zhang00} published a different camera calibration method only relying on two radial distortion parameters. A widely used implementation of the theoretical achievements of the above works is incorporated in Jean-Yves Bouguet's "Camera Calibration Toolbox for Matlab" \citep{calibration_bouguet}. Here a third order radial distortion parameter $k_3$ is introduced for higher accurracy. The computational model optimised for is described in \figref{eq:distortion}.

\begin{figure}[h!]
    \begin{equation*}
	\left[\begin{array}{c}
		u_{camera} \\
		v_{camera} \\
		w_{camera} \\
	\end{array} \right]
	=
    	\left[\begin{array}{c|c}
	    	{\bf \Omega} & {\bf \tau}
	\end{array} \right]
	\left[\begin{array}{c}
	    u \\
	    	v \\
	    w \\
	\end{array} \right]
    \end{equation*}
    
    \begin{equation*}
	{\bf x}_{normalized} = 
 	\left[\begin{array}{c}
	    \frac{ u_{camera} }{ w_{camera} } \\
	    \frac{ v_{camera} }{ w_{camera} } \\
	\end{array} \right]; 
	~r = length(	{\bf x}_{normalized} )
    \end{equation*}

    \begin{equation*}
	{\bf x}_{undistorted}
	=
	(1 + k_1 r^2 + k_2 r^4 + k_3 r^6 ) {\bf x}_{normalized} 
	~+~ 
	\left[\begin{array}{c}
	    2 p_1 {\bf x}_{normalized,x} {\bf x}_{normalized,y} + p_2 ( r^2 + 2{\bf x}_{normalized,x}^2) \\
	    p_1(r^2+2{\bf x}_{normalized,y}^2) + 2 p_2 {\bf x}_{normalized,x} {\bf x}_{normalized,y}\\
	\end{array} \right]
	\end{equation*}
    \begin{equation*}
		{\bf \tilde{x}}_{image} = {\bf\Lambda} ~ {\bf \tilde{x}}_{undistorted}
    \end{equation*}
    
    \caption{Distortion parameters in the pinhole camera model}
    \label{eq:distortion}
\end{figure}

The achievements of the toolbox were further developed in Intel's \citep{calibration_opencv} package . Additionally, the Graphics and Media Lab of the Lomonosov Moscow State University published a C++ and Matlab calibration toolbox known as \citep{calibration_gml}.
\citep{calibration_rgbdemo}

herrera 13


%http://burrus.name/index.php/Research/KinectCalibration
%vrui
%ros http://wiki.ros.org/kinect\_node/Calibration
%ellipses



\section{Super-resolution}
\label{sec:super_resolution}

% not depth:
Joint bilateral upsampling PCL: \citep{Kopf:2007}

The bilateral filter has been used previously for various image pro-
cessing tasks. Durand and Dorsey [2002] applied the bilateral fil-
ter to HDR tone mapping and also described a fast approximation,
which was recently improved upon [Paris and Durand 2006; Weiss
2006].
Ramanath and Snyder [2003] used the bilateral filter in the context
of demosaicking to improve edge sensitivity. Their method is re-
stricted to Bayer patterns with a fixed small upsampling factor, and
does not use a guidance image as we do.
%very close:
yang

\citep{MatsuoFI13} \\
\citep{} TUVienna paper of selfsimilarities

In \citep{Silberman:ECCV12} ... they give a very good kinect dataset for segmentation, use RANSAC and implement cross bilateral filtering



\section{Kinect fusion papers}
\label{sec:kinfu}

%vladlen:
\citep{Zhou:2013}
% mr siggraph 13 kinfu:
\citep{Chen:2013:Scalable_volumetric}
% kintinous 2
\citep{Whelan13iros}
% Kintinuous1
\citep{Whelan12rssw}
% lefloch
Point based Fusion:
\citep{keller13realtime}

Pose estimation improvement: \citep{Whelan13icra}
Multi scale depth: \citep{Fuhrmann:2011}  keywords = depth map integration, hierarchical signed distance field, marching tetrahedra, multi-scale depth map fusion, multi-view stereo depth maps, surface reconstruction


\section{<planned>}

%	\begin{enumerate}
%		\item 3D reconstruction using Multi-view stereo
%		\begin{itemize}
%			\item Curless and Levoy
%			\item Rusinkiewicz
%			\item Szeliski
%			\item Agarwal
%			\item Snavely
%			\item Seitz
%			\item Furukawa and Ponce
%			\item Strecha
%			\item etc. from literature review (got good marks for it)
%			\item Kim et al: Scene reconstruction from High Spatio-Angular Resolution Light Fields (Siggraph '13)
%		\end{itemize}
%
%		\item 3D reconstruction using RGB-D
%		\begin{itemize}
%			\item Voxelgrid based
%			\begin{itemize}
%				\item Newcombe: Kinect fusion
%				\item Whelan: Kintinuous and other loop closure papers
%				\item Vladlen: Dense scene reconstruction with points of interest
%				\item etc.
%			\end{itemize}
%
%			\item Height-field based, layered
%			\begin{itemize}
%				\item Adaptive voxelgrid based, scene scale aware (Freiburg)
%				\item etc.
%			\end{itemize}
%
%			\item Point based
%			\begin{itemize}
%				\item Damien Lefloch
%				\item etc.
%			\end{itemize}
%
%			
%		\end{itemize}
%			
% 		\item Pose estimation
%		\begin{itemize}
%			\item some ICP references
%			\item Gyro papers
%			\item MVS already earlier
%		\end{itemize}
%		
%		\item Upsampling
%		\begin{itemize}
%			\item RGB superresolution
%			\item Depth superresolution
%			\begin{itemize}
%				\item Non-guided
%				\begin{itemize}
%					\item Learning based
%						\begin{itemize}
%							\item Mac Aodha et al. '12
%							\item etc.
%						\end{itemize}
%					\item Other
%					\begin{itemize}
%						\item Hornacek et al.: Depth Super Resolution by Rigid Body Self-Similarity in 3D, CVPR '13
%						\item etc.
%					\end{itemize}
%				\end{itemize}
%				
%				\item Guided
%				\begin{itemize}
%					\item Joint Bilateral
%					\begin{itemize}
%						\item Yang '07
%						\item etc.
%					\end{itemize}
%					\item MRF based
%					\begin{itemize}
%						\item Diebel and Thrun
%						\item Schuon et al.
%						\item Park et al. '11
%						\item Choi et al. '12
%						\item Shen and Cheung CVPR '13 (Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras)
%						\item etc.
%					\end{itemize}
%					
%					\item Spatio-temporal filtering
%					
%				\end{itemize}
%			\end{itemize}
%		\end{itemize}
%		
%		\item Commercial 3D scanners
%		\begin{itemize}
%			\item MVS based: Photosynth, Adobe, etc.
%			\item Laser scanner systems (industrial)
%			\item Kinect based: Scanec, Reva, ReconstructMe, ...
%		\end{itemize}
%		
%		\item Seen/used calibration references
%		\item Subdivision references
%		\item Efficient raycasting references
%		\item (+ Categorize all the ref e-mails from Gabriel, Oisin, Reading groups, etc.)
%	
%	\end{enumerate}

\chapter{System design} 
\label{chp:my_system}

\par The targeted system attempts to solve the problem of 3D reconstruction of fine surface details in static scenes. We have seen various examples of sparse 3D reconstruction methods using the achievements of multi-view stereo, structure from motion and SLAM techniques in \secref{sec:SFM}. In recent years, a large leap has been made in dense 3D reconstruction by methods in \secref{sec:kinfu}. By designing this system we attempted to address inaccuracies produced by the above mentioned methods to refine high frequency details in small segments of the reconstructed models. \\

\par The system designed acknowledges the power of SLAM performed on the dense model incorporated in the areas of RGB-D based methods. We also recognize the power of other research areas, where inter-sensory information is used to enhance output quality. We hypothesize that there is information collected by the capturing system but is discarded too early in the reconstruction pipeline of a standard RGB-D system. We looked for methods that enable this information to be reintegrated into the results of the reconstruction at later processing steps. The solution was designed based on the alleged potential of iterative, cost-volume based joint bilateral filtering with sub-pixel accuracy in \citep{cvpr-07-qingxiong-yang}. \\

\begin{figure}[h!]\centering
    \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/sys_overview.png}
    \caption{System overview}
    \label{fig:sys_overview}
\end{figure}

\par In \figref{fig:sys_overview} the overview of the designed solution is outlined. The main steps of the targeted system are as follows: (1) RGB-D acquisition and pre-processing, (2) 3D reconstruction, (3) new input acquisition and processing, (4) detail up-sampling and (5) mesh enhancement. Step (3) involves pose estimation of the new input. Ideally, this step enables high resolution RGB images to convey more information about details of the reconstruction. The pose estimation of a captured, high-resolution RGB image can be solved using methods in the area of multi-view stereo, for example bundle adjustment. This step was simulated in our project by the medium-resolution RGB images captured by the Kinect camera, where the pose estimation was performed by the Kinect Fusion algorithm. We thereby show, that there is information in the original input feed that can be used to improve on the output of the 3D reconstruction system. Substantial work was performed to enable mesh enhancement in step (5). However, some problems originating from the fact, that a re-sampling occurs when a 2D projection is created from the estimated pose could not be overcome in the frames of this project. Knowledge from the domain of geometry processing is to be involved to successfully merge the details rediscovered by our solution into the 3D reconstruction.

\par The system outlined above enables iterative refinement of the reconstructed model by acquisition of new input containing new information about the desired details.

\section{Assumptions}
\label{sec:assumptions}

\par When designing a system solving a specific problem, it is important to firmly decide what assumptions are made about the input, what problems are targeted to be solved and what problems are out of the scope of the planned method. We target to design a system, that reconstructs fine surface details of static scenes with mostly Lambertian surfaces allowing offline post-processing. \\

\par The fine surface details of a static scene are to be reconstructed. The problems arising when dealing with dynamic scenes are out of scope for this solution. Rigidity is a common assumption in the field of 3D reconstruction both for multi-view stereo and structured lighting methods. An essential building block of our concept is the possibility to later acquire new data about missing details and integrate it into the reconstruction. Since pose estimation of latter acquisitions is a substantial problem on its own, the assumption about static scenes seems reasonable to make. \\

\par Surfaces with close-to Lambertian reflectivity are considered. During experiments refractive and reflective surfaces appeared in the scene, but the main focus of the system development addresses problems emerging around fine details on mostly Lambertian surfaces. \\

\par The solution will perform post-processing offline. The planned key contribution of the work is to acquire surface details at a finer level than current state-of-the art solutions are capable of. As generally in all fields of computer science, a trade-off between quality and speed has to be made. Thus, the planned solutions operate using offline processing and refinement steps. Precedence in similar projects of using offline processing as capabilities of cloud computing has been observed in applications as Adobe Catch \citep{AdobeCatch} or Microsoft Photosynth \citep{Photosynth}. State-of-the art large-scale RGB-D reconstruction algorithms also may operate off-line, as \citep{Zhou:2013}. \\

\section{3D reconstruction}

The task of accurate real-time mapping of complex and
arbitrary indoor scenes in variable lighting conditions was claimed to have been solved by \citep{Newcombe11}. They list using only a
moving low-cost depth camera and commodity graphics hardware to their contributions as well. There have been made many improvements to the method since it's initial publication detailed in \secref{sec:kinfu}. We base the hypothesis of our work on the claim, that RGB-D reconstruction's attention to the complexity of the scenes can be developed further. Therefore, during the design of the system this algorithm was treated as baseline, its results were reproduced, carefully inspected and an attempt was made to improve upon them. During the extensive research performed to assess the capabilities of derivatives of this algorithm to reconstruct the targeted fine details we did rarely come across projects giving reason to attempt to reproduce their results instead, and treat them as baseline. However, in the future point-based fusion concepts introduced by \citep{keller13realtime} is worth taking into account to improve the starting point of our algorithm.

\begin{figure}[h!]\centering
	\begin{minipage}[b]{0.33\linewidth}
		\includegraphics[width=\textwidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/kinectfusion_example.png}
		\caption{Image from \citep{KinectFuSDKExample}}
		\label{fig:kinectsdk_example}
	\end{minipage}
	\begin{minipage}[b]{0.33\linewidth}
		\includegraphics[width=\textwidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/kinfu_baseline00.png}
		\caption{Mesh reconstruction of scene recorded by us. Reconstruction using PCL's Kinfu with our modifications}
		\label{fig:kinfu_baseline}
	\end{minipage}
	\begin{minipage}[b]{0.33\linewidth}
		\includegraphics[width=\textwidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/lefloch_largescale_cropped.png}
		\caption{\citep{keller13realtime}}
		\label{fig:lefloch}
	\end{minipage}
\end{figure}



\subsection{System input}
\par The system design applied in RGB-D SLAM 3D reconstruction methods aligns well to our initial concept of deign of solution. This is true both in terms of processing pipeline and hardware and software tools applied. The scene acquisition happens using sensors providing both chromatic and distance information. It is a common assumption to use a low-cost, usually hand-held device with integrated sensors as the Microsoft Kinect or ASUS Xtion Pro. A calibrated system of a time-of-flight camera and an RGB camera is another usual system setup. We used a calibrated Microsoft Kinect device. \\

\par An RGB-D SLAM 3D reconstruction system operates with at least two input streams. A data stream containing chromatic information about the observed scene, usually encoded in 8-bits per channel, and another information source conveying information about the spatial relations of the scene encoded in some form of distance information.
\par The Microsoft Kinect is capable of conveying three channels of chromatic data encoded in a 24-bit data-stream. The most important available resolution settings with the used OpenNI Middleware are 640 x 480 pixels at \mytilde 30 frames per second and 1280 x 1024 pixels at \mytilde 15 frames per second. The quality of the RGB image seems to suffer from the problems Bayer filtering type methods introduce, as it can be seen in \figref{fig:kinect_rgb_zoom}. This introduces slight corner detection problems at calibration time, and more significant problems when applying the up-sampling algorithm as discussed in the limitations section \secref{sec:limitations}. 
\par The depth stream is available in the resolution 640 x 480 at \mytilde 30 frames per second. The critical analysis of the quality of the depth map has been researched in many projects, amongst others by modelling the noise of the sensor \citep{NguyenIL12}, or applying temporal filtering \citep{RGBZcamera}. It is considered out of the scope of the current project to pre-process the depth stream before applying it in the system pipeline. \\

\par To establish working volume boundaries and area of focus, the operating range and optimal scanning distance of the used depth acquisition device has to be known. According to \citep{Kinect_ms} the Kinect depth sensor has a minimum range of 800 mm and a maximum of 4 m. The Kinect for Windows Hardware can work in Near Mode providing a range of 500 mm to 3 m. By sampling the depth maps acquired during our data collection process it can be stated, that these characteristics can be achieved using the OpenNI Middleware as well, depths around 500 mm have been observed. 
\par In future work, the \citep{Kinect_nyko_zoom} might provide possibility to zoom in further on the scene and acquire more accurate depth data about fine details, meanwhile the distortion effect of the additional lens has to be evaluated as well.

\subsection{Processing algorithm}
\par An open-source implementation of the classical Kinect Fusion algorithm was employed to retrieve a smooth 3D reconstruction from the data acquired. The \citep{PCL} has implementation both for the original small to mid-scale algorithm named "Kinfu" and an implementation capable of handling scenes with larger extent called "Kinfu Large Scale". Due to the targeted proof-of-concept nature of our project the computationally slightly less intensive Kinfu implementation was used as starting point. The sub-project was recompiled to a separate, modifiable library allowing slight customisations to the algorithm (project "MyKinfuTracker" in the source code attachment). Algorithmic changes or implementation of later publications detailed in \secref{sec:kinfu} incorporating improvements to the concept are considered out of scope for this project.\\

\par The outline of PCL's Kinfu algorithm is as follows: All depth data streamed from a Kinect sensor is merged into a global implicit surface model stored in a fixed resolution voxel grid in the GPU's memory. In each voxel grid cell the distance to the closest surface is stored as a truncated signed distance function (TSDF). Subject to convention the two signs represent the inside respectively outside of the assumed non-degenerate mesh. 
\par Upon arrival of new depth data a 3D point vertex and normal map is generated using camera the intrinsics. Correspondence to the stored mesh represented by the stored TSDF is calculated using ray-casting and the point cloud represented by the vertex map is used to estimate the new camera pose with respect to the current model. This is performed using a coarse-to-fine iterative closest point algorithm. 
\par The point cloud is then regenerated using the new camera pose and the appropriate cells of the voxel grid are updated. A new vertex and normal map is generated to prepare for the next arriving frame. The update method has large impact on our project goals. In \secref{sec:3d_sys_output} the role is discussed further with other parameters of the algorithm. \\

\par By recompiling PCL's Kinfu solution to a stand-alone library we prepared the implementation for future modifications. This allowed us to experiment with the different parameters detailed in the next section. These were hard coded into the solution in order to enable CUDA code to perform optimally. The most important modification besides change of parameters was to turn off the initial bilateral filtering of the input depth to enable higher attention to detail. The effectivity of this modification was suppressed by the averaging nature of the TSDF update method. Since the rest of the implementation assumed filtered depth maps with no missing information, a bilateral filtering step was reintroduced, that fills only pixels with missing data. A more accurate solution would require missing data to be disregarded at later stages of the pipeline.

\subsection{Configuration and system output}
\label{sec:3d_sys_output}

The results of the reconstruction stored in the voxel grid as a TSDF volume can be exported to several output formats. A GPU implementation of the classical marching cubes algorithm \citep{Lorensen:1987} exports the mesh into a point cloud. A triangle mesh can also be exported using the same method and connecting neighbouring grid cells to faces. The storage format is greedy storing all three vertices for every face. This introduced algorithmic challenges when performing subdivision on the mesh at later stages of the pipeline. The results can also be visualized using ray-tracing and Phong shading. We used the triangle mesh as output of this system in the pipeline.

\par Configuration parameters can be used to tweak the system to yield more optimal results, a reconstruction with higher attention to detail. The most important parameters of the Kinect Fusion algorithm are voxel grid resolution, working volume size, truncation distance, maximum movement threshold and weight of new information.

\par The resolution of the voxel grid and working volume size has the highest impact on the accuracy of fine details in the reconstruction. The parameters used during reconstruction resulted in a spatial resolution of $3 m / 512$ or $3 m / 640$ meaning an average accuracy of 5.85 mm respectively 4.68 mm. This implied a $3m^3$ working volume represented by $512^3$ respectively $640^3$ voxels. Higher values were bounded by hardware constraints (GPU memory size). The quality of the final output of our system was compared relatively to the output of the 3D reconstruction in the \secref{sec:discussion_results}.

\par The truncation distance has an impact on the accuracy of cavities and narrow details. The importance of this parameter is relevant when performing the raycasting steps of the algorithm. Since our solution does not need real-time performance this value could be increased to result in results with higher accuracy. Thus the output quality is not limited by this parameter, but by the spatial resolution of the voxel grid.

\par The maximum movement threshold is relevant to the pose estimation step of the algorithm. This parameter ensures, that the pose estimation algorithm did find a good match between the model and the new frame. By increasing the tolerance for movement between frames, we would allow less accurate pose estimations and faster camera movement. By decreasing the value more information would get discarded.

\par As stated earlier, the TSDF update method is an important parameter in terms of the reconstruction quality. In the given implementation, the information conveyed by new measurements are averaged giving each measurement equal weight. On the \citep{KinectFuSDKExample} page they say, that the weight parameter {\it "...controls the temporal averaging of data into the reconstruction volume â€“ increasing makes the system a higher detailed reconstruction, but one which takes longer to average and does not adapt to change. Decreasing it makes the volume respond faster to change in the depth (e.g. objects moving), but is noisier overall."}. The equal weighting of all measurements has a large impact on the output introducing a strong smoothing effect discarding fine surface details and giving place to the hypothesis of our project.

\section{Depth super-resolution}

\par The base hypothesis of our project states, that there is information recorded by the sensors that is discarded early in the pipeline. We claim, that by identifying and reintegrating this information at later stages, it is possible to improve the reconstruction of fine surface details in comparison to existing 3D reconstruction methods. 

\par In \secref{sec:super_resolution} many image-, depth- and joint super-resolution methods were overviewed. The claimed power of the method published by \citep{cvpr-07-qingxiong-yang} served as basis for the main hypothesis of the project. In their works they demonstrate successful ten-fold up-sampling of noisy, low resolution time-of-flight depth images with the help of registered RGB images. Later works often refer to \citep{cvpr-07-qingxiong-yang} as super-resolution by joint bilateral filtering. However, the original method applies joint bilateral filtering in an iterative fashion on a cost-volume originally necessary for the stereo-matching problem targeted. The main hypothesis of cross-bilateral filtering says that neighbouring pixels that are spatially closer and similar in [colour] appearance are more reliable and thus to be given more weight when estimating a new value for the center pixel of the filtering kernel.
\par The two main contributions of their project emphasized in our solution is this neglected iterative nature of their method and the implementation of sub-pixel accuracy to the depth search range and its resolution. An overview of their algorithm can be seen in \figref{fig:yang_overview}. The full algorithm was reimplemented during the course of the project. Given its computational intensity, a full GPU implementation was developed further described in \secref{sec:implementation_details}

\begin{figure}[h!]\centering
    \includegraphics[width=0.4\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/yang_overview.png}
    \caption{System overview of \citep{cvpr-07-qingxiong-yang}}
    \label{fig:yang_overview}
\end{figure}

\subsection{System input}

\par The system takes two registered images as input. One of the images is the depth information targeted for up-sampling. This image is supposed not to contain details to the desired scale. The second input is a colour image conveying the extra information used for enhancement. By applying the method described, the depth image can be up-sampled to the same resolution as the colour image. The exact registration of the images has proven itself to be of high importance. The system is also sensitive to several parameters as input, described in the next subsections.

\subsection{Processing algorithm}
\par The depth image is up-sampled to the resolution of the RGB image, and
serves as the initial hypothesis of the depth information. In our implementation a nearest neighbour strategy is applied to reduce the number of possible sources of artifacts. 
\par Afterwards, an iterative refinement process is started. First a cost volume is built based on the current depth estimate. For every pixel, every depth hypothesis within a relatively narrow search range is compared to the current hypothesis. The cost of the investigated depth value depends on the distance from the current hypothesis subject to a cost function. The algorithm uses a truncated quadratic cost function to enable large depth discontinuities, meanwhile ensuring a smooth gradient search.
\par Then joint bilateral filtering is applied to the cost volume to handle artifacts in the proximity of large gradients. The necessity of this step is supported by an undesired fattening effect around edges. By applying bilateral filtering the compatibility of the new depth value to its neighbourhood is investigated. This compatibility is modulated by the spatial distance between the neighbouring pixel subject to a Gaussian fall-off, and the distance in appearance also subject to a Gaussian fall-off with different variance.
\par After filtering the cheapest new depth value is selected by a winner-take-all strategy. Then the cost of the two neighbouring hypotheses are used to perform a sub-pixel estimation procedure to enhance the resolution determined by the search step during the construction of the cost volumes.

\par The stop condition is not described in the original paper, but the published datasets allow us to conclude that around 130 iterations were performed on depth data in the 0..255 range. Since our depth images usually lie in a larger range of 0..3000, a minimum-average-difference stop condition was tested in the implementations. Finally we decided to fix the allowed change of a depth value based on the assumption, that we can assume a maximum change in mm-s given the targeted scene.

\subsection{Configuration and system output}

\par The algorithm is moderately sensitive to several parameters that need to be adapted to the domains of the current problem. The highest sensitivity is introduced through the joint bilateral filtering. 

\par The variance of the two Gaussian fall-offs are to be set. The current solution operates with fixed variances, and the adaptivity to the characteristics of the input data is accomplished through the fact, that the appearance information is taken into account during the filtering. High spatial variance can be interpreted as a larger smoothing filter granting influence to more distant neighbours in the filter kernel. Low spatial variance dampens the smoothing effect to the cost of the effectivity of the filtering. A future improvement to our algorithm is to automatically calculate the maximum necessary filter kernel size from this variance (an often seen heuristic is to set it to two times the standard deviation), currently it's set separately.
\par The variance of the Gaussian responsible for assigning weights to neighbours based on appearance information is also an input parameter. A high variance can be interpreted as being more allowing regarding the appearance of the neighbours. This is favourable if the quality of the colour input is low. Low variance means being very restrictive and assuming good input quality where object similarity is well captured by the colour information. The original algorithm applies an equally weighted gray scale conversion. In future work the effect of not discarding hue information should be investigated. Given the low quality of the high resolution RGB images returned by the Kinect's camera, the gray scale conversion was a necessary decision.

\par Bilateral filtering methods usually allow several iterations to be performed to handle data with higher noise levels. The effect of this can be translated to a larger kernel size and thus only one iteration of filtering was performed on each slice of the cost volume. This decision is also supported by the fact, that the whole algorithm relies on an iterative concept.

\par The iterative up-sampling algorithm relies on parameters determining the truncation of the cost function, the search range and its resolution and the fixed maximum iteration count implementing the stop condition. A lower truncation level of the quadratic cost function is desired to preserve large depth discontinuities in the scenes. The search range is quite statically determined by the depth domain of the scene, and is recalculated at each iteration to speed up computations. The range is set to contain the current depth spectrum padded by the truncation value of the cost function. 

\par The search step parameter determines the maximum possible accuracy of the method without the sub-pixel refinement. It was set to 1 mm in during experiments. A value inherited from the fact, that the input depth data is provided encoded as 16 bit integers with mm precision. The calculations in the pipeline are performed on float data, and due to the sub-pixel accuracy calculations, have not too large impact. The interpretation of the search step is, that a quadratic function is fitted using three samples spaced by the search step. This gives a hard to interpret notion of accuracy.

\par In the original paper there is no mention of representation errors or limitation of the sup-pixel accuracy estimation due to close to zero divisions. In our implementation a salt-pepper like noise was observed close to very large edges. Therefore a change limitation was introduced to bound the output of the sub-pixel estimation step to the range $\pm 1 \cdot search\_step$.

\par The output of the algorithm is demonstrated in the validation chapter \chpref{chp:validation}. The validity of the reimplementation is also tested on the datasets attached to the original publication.
        
\section{Combined system pipeline}
\label{sec:combined}

\par The designed system accomplishes the targeted high-resolution 3D reconstruction by combining the above outlined two systems, smooth 3D reconstruction and depth up-sampling. The overview of the constructed system is shown in \figref{fig:sys_overview}. A very important concept in the designed solution is the possibility of including new data to enhance parts of the reconstruction previously not sufficiently observed. This allows for a freedom to combine sensors and their characteristics. This theoretical design feature was tested by using the existing acquired data to check, whether there is unused information not picked up by the initial reconstruction. The pose estimation of the Kinect Fusion algorithm is used to render a new depth image of the reconstructed model from a simulated virtual viewpoint. Since the pose is known, the aligned RGB data can be used to up-sample the depth from the virtual viewpoint.

\subsection{Pre-processing the input}

\par To ensure high quality alignment required by the later parts of the pipeline a extensive work was performed to gather information about the extrinsic and intrinsic parameters of the recording device. 10-component intrinsic parameters for both lenses were calculated alongside with the estimation of the extrinsics connecting the two cameras. Further details are discussed in \secref{sec:calibration}.

\par The estimated camera parameters are used to remove lens distortion from both input streams and to reduce the two coordinate systems to one by mapping the viewpoint of the depth camera to the coordinate space of the RGB camera. By doing this before the streams are provided to the smooth reconstruction algorithm a compromise is made. The mapping calculations introduce estimation errors, loss of depth data by occlusion in the new viewpoint and representation problems due to the non-rectangular output of the distortion removal algorithms. The gain is to be able to use the aligned input feeds for further, quality enhancing pre-processing algorithms, for example joint-bilateral filtering. The strong smoothing effect of the 3D reconstruction algorithm applied takes away the edge of this gain, in future work the viewpoint mapping is to be moved to after the smooth reconstruction step.

\subsection{Rendering a virtual viewpoint}

\par After the smooth 3D reconstruction, any virtual viewpoint of the system has to be possible to render. Therefore, the results of the reconstruction have to be stored. Furthermore the projection parameters of the virtual viewpoint have to match the characteristics of the capturing device that the newly acquired data was recorded with. The actual rendering of the model representation has to be performed, and information about the sampled parts of the mesh has to be collected.

\par The calculation of the actual position of the virtual viewpoint is detailed in the future work section \secref{sec:future_work}. Bundle adjustment implemented in \citep{SnavelySS06} or \citep{vsfm} can be used to align the new RGB data with the available snapshots from the Kinect stream. The new sparse and stored dense reconstruction model can than be aligned, so that the estimated new camera pose can be transformed between the two coordinate systems.

\par To  store the results of the smooth reconstruction in a loss-less fashion the data structure of the Kinfu implementation is to know. The 3D voxel grid is mapped to a 2D storage array in the GPU memory. In order to be able to perform a large variety of sampling operations on the results the original grid was saved to disk using existing tools. The read-back to GPU memory had to be implemented. The exact storage of the TSDF volume enables ray-casting to be performed to render the virtual viewpoint. Using the marching cubes algorithm the theoretical accuracy of this representation is approximated. However by storing triangle meshes in PLY format a wider range of existing rendering tools became available and calculation times were heavily reduced. This practical separation of the pipeline can be reversed in future work. The loss of accuracy was attempted to be counter-weighted by the employment of different mesh subdivision algorithms. The sub-optimal data structure of the triangle mesh is not compatible with the available existing subdivision implementations. Since this algorithmic step will prove unnecessary in future work, further attention was not given to the problem arising through an earlier approximation step.

\par To render the virtual viewpoint of the reconstructed model three different approaches were developed. PCL's octree implementation allowed the quick development of a ray-casting algorithm. The execution was performed on the CPU very similarly to the Kinfu GPU algorithm, thus the projection parameters involving the used camera intrinsics were closely controlled. The information about visible vertices and faces is easily retrievable using this method as well. This information is needed by the mesh enhancement step, where the modified primitives have to be merged with the unmodified primitives of the mesh. Unfortunately our meshes consisted of 1.5 - 2.5 million vertices which made the CPU implementation too slow despite the effective octree point cloud representation. Problems arised in form of the hand-tuning of the maximum octree grid resolution and the need to account for larger faces occluding rays but not represented by the octree structure.

\par GPU based renderers of triangle meshes are widely known. PCL relies on the library called \citep{vtk}. Unfortunately in its newest version the possibility to specify all elements of the projection matrix independently is not enabled. Therefore, after many attempts to use the offered interface to accomplish the incorporation of the exact intrinsics into the rendering system a third method had to be discovered. Visibility information is also not retrievable using this method.
\par A GLSL based rendering environment was implemented to ensure full control over the projection parameters, quick rendering of the large number of input triangles, and full information about appearing vertex- and face ids. A FramebufferObject is set as render target, the depth buffer texture is read and rescaled to retrieve the depth map, and unclamped unsigned textures are attached as fragment shader outputs to retrieve information about sampled vertices and faces.

\subsection{Model enhancement}
\par Several strategies were developed to accomplish the substitution of the projected vertices and faces by the up-sampled depth data. It is established, that the re-sampling nature of the problem requires solutions to be imported from the geometry processing domain. Operations like this were considered out of scope for the current project.

\par As resul

\section{Calibration}
\label{sec:calibration}
During the development Determining the necessity of accurate intrinsic and extrinsic calibration of the recording system

What is calibration, pinhole camera model, different notations (oulu, etc.), 
with IR, without IR
USB bus problems, overlayed image

% Input rgb and depth
%\begin{figure}[h!]\centering
%    \begin{minipage}[b]{0.49\linewidth}
%        \includegraphics[width=\textwidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/SuperRes-NI-1-5/build/out/imgs_20130725_1809/img8_00000001.png}
%        \caption{Input RGB}
%    \end{minipage}
%    \begin{minipage}[b]{0.49\linewidth}
%        \includegraphics[width=\textwidth]{/media/Storage/Dropbox/UCL/project/results/presentation_300713/dep8_00000001.png}
%		\caption{Input depth}
%    \end{minipage}
%\end{figure}
%
%% Simple alignment
%\begin{figure}[h!]\centering
%	\begin{minipage}[b]{0.49\linewidth}
%		\includegraphics[width=\textwidth]{/media/Storage/Dropbox/UCL/project/results/presentation_300713/dep16AndRgb_00000001.png}
%		\caption{No alignment}
%	\end{minipage}
%	\begin{minipage}[b]{0.49\linewidth}
%		\includegraphics[width=\textwidth]{/media/Storage/Dropbox/UCL/project/results/presentation_300713/dep16AndRgb_00000000.png}
%		\caption{"Built-in alignment"}
%	\end{minipage}
%\end{figure}
%
%
%\begin{figure}[h!]\centering
%    \begin{minipage}[b]{0.49\linewidth} \label{fig:ir}
%        \includegraphics[width=\textwidth]{/media/Storage/Dropbox/UCL/project/results/presentation_300713/irAndDep8_00000001.png}
%        \caption{Default IR and depth overlay}
%    \end{minipage}
%    \begin{minipage}[b]{0.49\linewidth}
%        \includegraphics[width=\textwidth]{/media/Storage/Dropbox/UCL/project/results/presentation_300713/offsIrAndDep8_00000001.png}
%        \caption{IR offset by radius of convolution kernel}
%    \end{minipage}
%\end{figure}
%
%\begin{figure}[h!]\centering
%        \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/results/blendedUndistortedUC3x2.png}
%        \caption{Mapping from 640x480 depth to 1280x1024 RGB after calibration and distortion removal}
%\end{figure}

%//TODO: %


default image, built in alignment image, good image\\
calibration papers, and their comparison of optimisation techniques\\
calibration with different sources (depth and acryl, ir, ellipses)\\
cuda mapping\\


\subsection{takeaway}
Ir image with and without projector\\
large checkerboard vs. small checkerboard\\
all positions (mosaic image?)\\
checkerboard sze 
All the squares must be clearly visible (unoccluded).
Use a tripod
Take 25 images and more
Use a paper size ".3" and more
Square size is 3-5 cm
The chessboard on the images must be located in the all places of the camera matrix


\section{Implementation details}
\label{sec:implementation_details}

\par Besides a single core C++ implementation, a less optimised, GPU based parallel implementation was created using the NVIDIA's Thrust library. Since both of these kept the run times for a single frame in the hour magnitude at the resolution of 1280 x 1024, a more optimised CUDA implementation was also developed. This implementation brought down run-times to 8-15 minutes on 

\subsection{Drivers}
\subsection{Third-party libraries}
simultaneous rgb and IR problems
NVIDIA bilateralfiltering sample
PCL
 pcl bilateral upsampling
 \citep{DCBGridStereo}
 
%https://github.com/jtbates/lua-kinect/blob/master/depth_to_point_cloud_par.cu
 OpenCV
 Eigen
 Thrust
 OpenNI, freeglut
 
Renderings made using \citep{Meshlab}.

%%% CHAPTER 5 %%%
\chapter{Validation / experiments}
\label{chp:validation}

%%% SECTION %%%
\section{Structure from motion} 
\label{sec:sfm}

% acerlit image
\myfig
{acerlit_1}
{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/acer_lit00706.jpg}
{A frame of video of backside of a monitor}
{acerlit_mesh}
{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/acer_lit01.png}
{Visual SFM 3D reconstruction of monitor} 

% oil bottle image
\begin{figure}[h!]\centering
	\begin{minipage}[b]{0.49\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/ob00003r2.jpg}
		\caption{A frame of video of oil bottle}
		\label{fig:ob_1}
	\end{minipage}
	\begin{minipage}[b]{0.49\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/ob_mesh00.png}
		\caption{Visual SFM 3D reconstruction of oil bottle}
		\label{fig:ob_mesh}
	\end{minipage}
\end{figure}

%%% SECTION %%%
\section{Iterative joint bilateral upsampling with sub-pixel accuracy}
\label{sec:yang}

\begin{figure}[h!]\centering 
        \includegraphics[width=\linewidth]{/media/Storage/Dropbox/UCL/project/results/final/yang_checkerboard/yanged_nonflat.png}        
        \caption{Example run of implementation on checkerboard scene, results without and with texture after 50 iterations.}
        \label{fig:yang_checkerboard}
\end{figure}

In \figref{fig:yang_checkerboard}

Yang reproductions

Yang does not introduce new depths

%%% SECTION %%%
\section{Results}
\label{sec:results}

\begin{figure}[h!]\centering
    \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/keyboard_192_final.png}
    \caption{Close up rendering of keyboard upsampled from skew view angle}
    \label{fig:keyboard_192}
\end{figure}

\begin{figure}[h!]\centering
    \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/keyboard2_192_final.png}
    \caption{Close up rendering of different keyboard upsampled from closer to normal view angle}
    \label{fig:keyboard2_192}
\end{figure}

See meshes on CD

\chapter{Discussion} 
\label{chp:discussion}

\section{Results}
\label{sec:discussion_results}

    - come back to 3's experiments\\
    - interpret experiments\\
    - did you notice, that 2 were good, and 3rd was abd, remember what I sadi in 3.2, that kinfu does it's calibration...well ,that's crap here (threshold), it happend to be good for 2 sequences, but not their thirds \\
-     Pros, cons \\
- PFM format \\

\subsection{Dense or sparse reconstruction}

\par An easy to operate, hand-held scanning device is used. The developed system will most likely use a smartphone with built in inertial measurement unit, gyroscope, magnetometer and GPS sensor. Alternatively a camera with mounted and calibrated motion and orientation sensors can be used. The performance of structured lighting devices as Microsoft Kinect \cite{Kinect} or Asus Xtion PRO \cite{XtionPro} will also be explored. Eventually a higher resolution device as Leap-motion's gesture recognition solution \cite{LeapMotion} will be evaluated. To ensure interchangeability of the sensors, and to get these systems to cooperate to the highest level possible, a well designed framework needs to be developed. The extensibility of the KinectFusion SDK will be tested once it get's released as part of the Windows 8 SDK as promised \cite{SDKKinectFusion}. Alternatives, as the implementation of a Kinect Fusion like functionality in the Point Cloud Library ("Kinfu") will also be tested \cite{KinFu}.
\par The commercial accessibility of the used sensor setup has a slightly increased emphasis at this stage of the project. Low cost and out of the box operability would open the possibilities for crowd sourcing in follow-up projects, something that has been targeted by many other research labs as Microsoft (at Techfest 2013) or the University of Washington (Photocity, Pointcraft).

/// The current sensor pose is simultaneously obtained by
tracking the live depth frame relative to the global model using a
coarse-to-fine iterative closest point (ICP) algorithm, which uses
all of the observed depth data available. We demonstrate the advantages
of tracking against the growing full surface model compared
with frame-to-frame tracking, obtaining tracking and mapping results
in constant time within room sized scenes with limited drift
and high accuracy. We also show both qualitative and quantitative
results relating to various aspects of our tracking and mapping system.
Modelling of natural scenes, in real-time with only commodity
sensor and GPU hardware, promises an exciting step forward
in augmented reality (AR), in particular, it allows dense surfaces to
be reconstructed in real-time, with a level of detail and robustness
beyond any solution yet presented using passive computer vision.

\section{Limitations} 
\label{sec:limitations}
	- kinfu memory needs
	- so if Kinfu is not reliable, say it! so if it does not work (pose) than \citep{Whelan13icra} \\
	- misalignment, few frames later, it's great \\
	- calibration \\
    - obvious (Yang on chessboard, it still does) \\
    - summarizing what I discovered \\
    - Kinect RGB quality \\
    \begin{figure}[h!]\centering
        \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/kinect_rgb_zoom.png}
        \caption{Kinect RGB image quality at 1280 x 1024 resolution}
        \label{fig:kinect_rgb_zoom}
    \end{figure}
    - TODO: aligned kinect-rgb, but misaligned kinfu frame example
    - Yang hand-tuned parameters, rgb grayscale conversion, automatic spatial sigma, 
    - Yang no new depth
yang:     guided filtering problem of inverted edges
    
\section{Future work}
\label{sec:future_work}

- Using segmentation \citep{Silberman:ECCV12} \\
- Occlusion \citep{Hoiem:2011} \\
- Learning based alignment and depth \citep{Herrera:LearnedJointMRF} \\
- put back into mesh (loookup papers!)
- geom processing papers from PCL surface package

This concept fits well into the idea of using a smartphone as sensor in terms of storage space even when recordings of movement sensors' measurements are made. It also fits the concept of an eventual crowd sourcing follow-up project. 

future work: I advocate Yang for the following reasons, or I discourage, since...or in some situations this, than that \\
Kinect \\
Yang \\
Grid recordings of monitor\\

    - zoom lense: \citep{Kinect_nyko_zoom}
In future work, the \citep{Kinect_nyko_zoom} might provide possibility to zoom in futher on the scene and acquire more accurate depth data about fine details, meanwhile the distortion effect of the additional lens has to be evaluated.

- kinfu update weight experiments
-Yang params by training

- tsdf ray-casting instead of mesh

\par The calculation of the actual position of the virtual viewpoint is detailed in the future work section \secref{sec:future_work}. Bundle adjustment implemented in \citep{SnavelySS06} or \citep{vsfm} can be used to align the new RGB data with the available snapshots from the Kinect stream. The new sparse and stored dense reconstruction model can than be aligned, so that the estimated new camera pose can be transformed between the two coordinate systems.


%\section{System plan}
%	\begin{itemize}
%		\item Calibration
%			\begin{itemize}
%				\item Depth image based
%				\item IR image based
%				\item Undistort
%			\end{itemize}
%		\item Low resolution 3D reconstruction
%			\begin{itemize}
%				\item Multi View Stereo (VSFM, Bundler, Photosynth, etc.)
%				\item Kinect Fusion, PCL::Kinfu
%			\end{itemize}
%		\item Pose estimation of new input
%			\begin{itemize}
%				\item VSFM ( for RGB input )
%				\item ICP ( for RGB-D input )
%				\item Gyro+IMU (a noisy graph of acquired data)
%			\end{itemize}
%		\item Mesh subdivision (Linear, Butterfly, Catmull-Clark)
%		\item 2D projection
%			\begin{itemize}
% 				\item Raycasting using Octree
%				\item GLSL projection
%			\end{itemize}
%		\item Yang filtering
%			\begin{itemize}
%				\item Iterative Cross Bilateral filtering
%				\item Subpixel accurracy
%			\end{itemize}
%		\item Mesh enhancement by backprojection
%	\end{itemize}
%	
%\section{System design}
%	\begin{itemize}
%		\item Implementation details and takeaway experience of "System plan" elements
%		\item Merge into previous?
%	\end{itemize}
%	
%\section{Evaluation}
%	\begin{itemize}
% 	\item Experiments
%		\begin{itemize}
%			\item Calibration
%				\begin{itemize}
%					\item Kinect built in calibration
%					\item Bogouet calibration with and without lens distortion (PARAMETERS explained)
%					\item Undistort effectivity (project undistorted depth map to 3D)
%				\end{itemize}
%				
%			\item Filtering
%				\begin{itemize}
%					\item CrossBilateral filter vs. Bilateral
%					\item Yang vs. CrossBilateral (PARAMETERS explained)
%					\item Trilateral, Guided, Pixel Weighted Average Strategy (Garcia et al,ImProc, 2010) - {\bf IF there's time}
%				\end{itemize}
%			\item Kinect fusion related
%			\begin{itemize}
%				\item Kinfu vs. Kinfu w/ filtering turned OFF
%				\item Kinfu voxel grid resolution ($386^3$,$512^3$,$640^3$)
%				\item Kinfu + Yang( original kinect depth frames )
%				\item Kinfu + Yang( "arbitrary pose" )
%			\end{itemize}
%		\end{itemize}
%	\item Existing libraries used
%	\item Data capturing details
%	\item No existing datasets compared (no time)
%
%	\end{itemize}
%
%\section{Results}
%\begin{itemize}
%	\item Upsampling with original depth frames
%	\item Upsampling with simulated "arbitrary pose"
%
%	\item original yang images
%	\item subpixel refinement
%	
%	\item So, Yang is is not suitable for this resolution, it only could improve, when the input was much noisier
%	\item Different approaches can be plugged in to this framework to enhance depth
%\end{itemize}
%\section{Conclusions, Future work}
%	\begin{itemize}
%		\item 3D reconstruction improvements
%			\begin{itemize}
%				\item Improve on smoothing effect of voxel grid weights
%				\item Pose estimation improvements
%			\end{itemize}
%		\item RGB high-resolution capture, vsfm pose estimation 
%		\item Enhancement by best pose selection from input image collection for given pixel/region
%	\end{itemize}
 

\citep{Lowe04}

%\bibliographystyle{plainnat}
\bibliography{litrev}
	
\end{document}


\lstset{style=customcpp}
%\begin{lstlisting}
%__device__ float yangRangeDist( float4 a, float4 b, float sigma )
%{
%    float mod = ( fabs(b.x - a.x) +
%                  fabs(b.y - a.y) +
%                  fabs(b.z - a.z)  ) / 3.f;
%    return __expf(-mod / sigma);
%}
%template <typename T>
%__global__ void
%d_cross_bilateral_filterF( T *dOut, int w, int h, size_t outPitch,
%                           float range_sigma, int r, bool onlyZeros = false )
%{
%    int x = blockIdx.x*blockDim.x + threadIdx.x;
%    int y = blockIdx.y*blockDim.y + threadIdx.y;
%
%    if (x >= w || y >= h) return;
%
%    float sum = 0.f, factor = 0.f, t = 0.f;
%    float4 guideCenter = tex2D( guideTex, x, y );
%    T      centerPix   = fetchTexture<T>( x, y );
%
%    // check for early exit
%    if ( onlyZeros && (centerPix != 0.f) )
%    {
%        dOut[y * outPitch + x] = centerPix;
%        return;
%    }
%
%    // estimate cost volume
%    for ( int i = -r; i <= r; ++i )
%    {
%        for ( int j = -r; j <= r; ++j )
%        {
%            // read depth
%            T curPix = fetchTexture<T>( x+j, y+i );
%            // skip, if no data
%            if ( onlyZeros && curPix == 0.f )
%                continue;
%            // read rgb
%            float4 guidePix = tex2D( guideTex, x + j, y + i );
%            // estimate weights
%            factor = cGaussian[i + r] * cGaussian[j + r] *
%                     yangRangeDist( guidePix, guideCenter, range_sigma );
%            // accumulate
%            t   += factor * curPix;
%            sum += factor;
%        }
%    }
%    if ( sum > 0.f )
%        dOut[y * outPitch + x] = t / sum;
%    else
%        dOut[y * outPitch + x] = centerPix;
%}
%
%\end{lstlisting}


