\documentclass{ucl_thesis}

%twoside for double page printing
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{color}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage{algorithmic}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[square]{natbib}

\usepackage[font=footnotesize,labelfont=bf,singlelinecheck=on]{caption}
\usepackage[hyphens]{url}

\usepackage{amsmath}
% Uncomment this to activate the TikZ library, which is useful for fancy block-diagrams.
%    \usepackage{tikz}
%    \usetikzlibrary{positioning,arrows,shapes.misc}
\usepackage{multirow}

%\numberwithin{algorithm}{chapter}
%\usepackage{epsf}
\usepackage{fancyvrb}

%\documentclass[12pt,a4paper]{article}
%\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
%\usepackage{amsmath}
%\usepackage{amsfonts}
%\usepackage{amssymb}
%\usepackage{graphicx}
%usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
%
%\usepackage[]{natbib}

% // TODO: add pagenumbers

%% LISTINGS
\usepackage{color}
\usepackage{listings}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\definecolor{purple}{rgb}{0.28,0.23,0.5}
\lstdefinestyle{customcpp}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  frame=L,
  xleftmargin=\parindent,
  language=C++,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\bfseries\color{green},
  commentstyle=\itshape\color{purple},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
  numbers        = left,
  stepnumber     = 5,
}
%%

%% figure template
\newcommand{\myfig}[6]{%
\begin{figure}[h!]\centering%
	\begin{minipage}[b]{0.49\linewidth}\centering%
		\includegraphics[width=\textwidth]{#2}%
		\caption{#3}%
		\label{fig:#1}%
	\end{minipage}%
	\begin{minipage}[b]{0.49\linewidth}\centering%
		\includegraphics[width=\textwidth]{#5}%
		\caption{#6}%
		\label{fig:#4}%
	\end{minipage}%
\end{figure}%
}
%%

%% shortcuts
%Malcolm added some custom commands here - you can make your own!
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\figref}[1]{(Fig. \ref{#1})}
\newcommand{\secref}[1]{(Section \ref{#1})}
\newcommand{\chpref}[1]{(Chapter \ref{#1})}
\def\etal{{et~al.}}
\def\ie{{\it i.e.,\ }}
\def\etc{{\it etc.,\ }}
\def\eg{{\it e.g.,\ }}
\def\vs{{\it vs.\ }}
\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
%%

\author{Aron Monszpart}
\title{Hand-held Scanning of Surface Details}
\def \supervisor {Dr. Gabriel Brostow}
\date{September 2013}

\begin{document}

\bibliographystyle{plainnat}
\maketitle
\numberwithin{algorithm}{chapter}
\setcounter{page}{1}
\pagenumbering{roman}
\pagestyle{plain}

% Notice how the star "*" is used throughout LaTex to modify the normal behavior. For example, \section* instead of \section 
% tells LaTex NOT to number this section.


\newpage
\section*{Abstract}
I've done some stuff
%\input{abstract.tex}


\newpage
\section*{Acknowledgements}
Dr. Gabriel Brostow, Dr Neill Campbell, Dr. Oisin Mac Aodha, Fabrizio Pece, Malcolm Reynolds, Clement Godard, Peter Rennert and the UCL PRISM group

%\setcounter{tocdepth}{2}
\tableofcontents
\listoffigures
%\listoftables
%\listofalgorithms
\newpage

\setcounter{page}{1}
\pagenumbering{arabic}
\pagestyle{plain}

\chapter{Introduction and Background} 

\par The targeted system attempts to solve the problem of 3D reconstruction of fine surface details in static scenes. We have seen various examples of sparse 3D reconstruction methods using the achievements of multi-view stereo, structure from motion and SLAM techniques in \secref{sec:SFM}. In recent years, a large leap has been made in dense 3D reconstruction by methods in \secref{sec:kinfu}. By designing this system we attempted to address inaccurracies produced by the above mentioned methods to refine high frequency details in small segments of the reconstructed models.

3D recon bevezetes
Depth sr bevezetes

\label{chp:background}
	\begin{itemize}	
		\item Motivation
		\item Overview of approach
		\begin{itemize}
			\item Calibration
			\item Low resolution 3D reconstruction
			\item Pose estimation of new input
			\item Mesh subdivision and 2D projection
			\item Depth upsampling
			\item Mesh enhancement by backprojection
		\end{itemize}
	\end{itemize}

\chapter{Related work} 
\label{chp:related_work}

\section{Multi view stereo}
\label{sec:mvs}
Spatioangular lightfields, coarse-to-fine approach, denser reconstruction disney: \citep{Kim:2013}

\section{Calibration}
\label{sec:lit_calib}

One of the initial steps of the setup of every multi-camera system has to be calibration. The expression calibration can be interpreted with several meanings. In this document we refer to calibration as the preprocessing step that captures the characteristics of the physical recording system accounting for, amongst others, inaccuracies in the physical manufacturing processes of the cameras. A successful calibration enables a correspondence to be calculated between data captured at the same time with the different cameras. This correspondence is data dependent in the case of depth cameras, and thus calculated every frame.\\

A wider interpretation of calibration includes the calculation of a nearly global position of the recording system. It usually is incorporated by the relative position estimation between the observed scene and the recording system, which can be viewed as global in case the global position of the observed scene is known. \\

Most previous works targeting to develop a highly accurate calibration system agree on the computational parameter model called pinhole-camera model \figref{eq:pinhole}. The model specifies the parameters: focal length ($f_x$, $f_y$), principal point ($c_x$, $c_y$) and skew ($\gamma$).

\begin{figure}[h!]
\begin{equation}
\lambda
\underbrace{
	\left[\begin{array}{ccc}
	    x \\
	    y \\
	    1 \\
	\end{array} \right]
}_{
	{\bf \tilde{x}}_{image}
}
%
=
%
\underbrace{
	\left[\begin{array}{cccc}
			f_x & \gamma & c_x & 0 \\
			0   & f_y    & c_y & 0 \\
	 		0   & 0      & 1   & 0 \\
	\end{array} \right]
}_{
	\left[\begin{array}{cc}
		\bf{\Lambda} & \bf{0}
	\end{array} \right]
}
%
\underbrace{
	\left[\begin{array}{cccc}
	    r_{1,1} & r_{1,2} & r_{1,3} & t_1 \\
	    r_{2,1} & r_{2,2} & r_{2,3} & t_2 \\
	    r_{3,1} & r_{3,2} & r_{3,3} & t_3 \\
	    0       & 0       & 0       & 1 \\
	\end{array} \right]
}_{
	\left[\begin{array}{cc}
		\bf{\Omega}     & \bf{\tau} \\
		\bf{0}^{\it{T}} 	& 1		      \\
	\end{array} \right]
}
%
\underbrace{
	\left[\begin{array}{c}
	    u \\
	    v \\
	    w \\
	    1 \\
	\end{array} \right]
}_{
	{\bf \tilde{x}}_{world}
}
\end{equation}
\caption{Correspondence between a point in global world coordinates (X,Y,Z) and camera image coordinates (u,v) according to the pinhole camera model. Source: UCL 2012 Machine Vision lecture notes. }
\label{eq:pinhole}
\end{figure}

Initial works \citep{Melen:1994}, \citep{Weng:1992} use the combination of linear, and non-linear minimisation techniques to calculate physically based camera parameters. In [9] non-physical implicit parameters are used in a two-step optimisation approach. The baseline calibration technique used in this project relies on the four-step calibration approach presented by \citep{Heikkila:1997}. They introduced a technique to estimate a four component set of distortion parameters in addition to the intrinsics of the pinhole camera. These parameters account for the radial ($k_1$, $k_2$) and tangential ($p_1$, $p_2$) distortion of the camera model. In parallel \citep{Zhang00} published a different camera calibration method only relying on two radial distortion parameters. A widely used implementation of the theoretical achievements of the above works is incorporated in Jean-Yves Bouguet's "Camera Calibration Toolbox for Matlab" \citep{calibration_bouguet}. Here a third order radial distortion parameter $k_3$ is introduced for higher accurracy. The computational model optimised for is described in \figref{eq:distortion}.

\begin{figure}[h!]
    \begin{equation*}
	\left[\begin{array}{c}
		u_{camera} \\
		v_{camera} \\
		w_{camera} \\
	\end{array} \right]
	=
    	\left[\begin{array}{c|c}
	    	{\bf \Omega} & {\bf \tau}
	\end{array} \right]
	\left[\begin{array}{c}
	    u \\
	    	v \\
	    w \\
	\end{array} \right]
    \end{equation*}
    
    \begin{equation*}
	{\bf x}_{normalized} = 
 	\left[\begin{array}{c}
	    \frac{ u_{camera} }{ w_{camera} } \\
	    \frac{ v_{camera} }{ w_{camera} } \\
	\end{array} \right]; 
	~r = length(	{\bf x}_{normalized} )
    \end{equation*}

    \begin{equation*}
	{\bf x}_{undistorted}
	=
	(1 + k_1 r^2 + k_2 r^4 + k_3 r^6 ) {\bf x}_{normalized} 
	~+~ 
	\left[\begin{array}{c}
	    2 p_1 {\bf x}_{normalized,x} {\bf x}_{normalized,y} + p_2 ( r^2 + 2{\bf x}_{normalized,x}^2) \\
	    p_1(r^2+2{\bf x}_{normalized,y}^2) + 2 p_2 {\bf x}_{normalized,x} {\bf x}_{normalized,y}\\
	\end{array} \right]
	\end{equation*}
    \begin{equation*}
		{\bf \tilde{x}}_{image} = {\bf\Lambda} ~ {\bf \tilde{x}}_{undistorted}
    \end{equation*}
    
    \caption{Distortion parameters in the pinhole camera model}
    \label{eq:distortion}
\end{figure}

The achievements of the toolbox were further developed in Intel's \citep{calibration_opencv} package. Additionally, the Graphics and Media Lab of the Lomonosov Moscow State University published a C++ and Matlab calibration toolbox known as \citep{calibration_gml}. Other projects use the \citep{calibration_rgbdemo} package for calibration. Modeling of the noise characteristics of the range camera allows even more accurate calibration presented by \citep{calibration_herrera}. Further improvements using additional prior knowledge is presented in \citep{Herrera:LearnedJointMRF}.


\section{Super-resolution}
\label{sec:super_resolution}

% not depth:
Joint bilateral upsampling PCL: \citep{Kopf:2007}

The bilateral filter has been used previously for various image pro-
cessing tasks. Durand and Dorsey [2002] applied the bilateral fil-
ter to HDR tone mapping and also described a fast approximation,
which was recently improved upon [Paris and Durand 2006; Weiss
2006].
Ramanath and Snyder [2003] used the bilateral filter in the context
of demosaicking to improve edge sensitivity. Their method is re-
stricted to Bayer patterns with a fixed small upsampling factor, and
does not use a guidance image as we do.
%very close:
yang

\citep{MatsuoFI13} \\
\citep{} TUVienna paper of selfsimilarities

In \citep{Silberman:ECCV12} ... they give a very good kinect dataset for segmentation, use RANSAC and implement cross bilateral filtering

\citep{guided_filter}



\section{Kinect fusion papers}
\label{sec:kinfu}

%vladlen:
\citep{Zhou:2013}
% mr siggraph 13 kinfu:
\citep{Chen:2013:Scalable_volumetric}
% kintinous 2
\citep{Whelan13iros}
% Kintinuous1
\citep{Whelan12rssw}
% lefloch
Point based Fusion:
\citep{keller13realtime}

Pose estimation improvement: \citep{Whelan13icra}
Multi scale depth: \citep{Fuhrmann:2011}  keywords = depth map integration, hierarchical signed distance field, marching tetrahedra, multi-scale depth map fusion, multi-view stereo depth maps, surface reconstruction

streams to provide a better pose estimation \citep{Whelan13icra}. The possibility of applying machine learning methods for more accurate pose estimation is also to be explored \citep{malisiewicz-iccv11}.


\section{<planned>}

%	\begin{enumerate}
%		\item 3D reconstruction using Multi-view stereo
%		\begin{itemize}
%			\item Curless and Levoy
%			\item Rusinkiewicz
%			\item Szeliski
%			\item Agarwal
%			\item Snavely
%			\item Seitz
%			\item Furukawa and Ponce
%			\item Strecha
%			\item etc. from literature review (got good marks for it)
%			\item Kim et al: Scene reconstruction from High Spatio-Angular Resolution Light Fields (Siggraph '13)
%		\end{itemize}
%
%		\item 3D reconstruction using RGB-D
%		\begin{itemize}
%			\item Voxelgrid based
%			\begin{itemize}
%				\item Newcombe: Kinect fusion
%				\item Whelan: Kintinuous and other loop closure papers
%				\item Vladlen: Dense scene reconstruction with points of interest
%				\item etc.
%			\end{itemize}
%
%			\item Height-field based, layered
%			\begin{itemize}
%				\item Adaptive voxelgrid based, scene scale aware (Freiburg)
%				\item etc.
%			\end{itemize}
%
%			\item Point based
%			\begin{itemize}
%				\item Damien Lefloch
%				\item etc.
%			\end{itemize}
%
%			
%		\end{itemize}
%			
% 		\item Pose estimation
%		\begin{itemize}
%			\item some ICP references
%			\item Gyro papers
%			\item MVS already earlier
%		\end{itemize}
%		
%		\item Upsampling
%		\begin{itemize}
%			\item RGB superresolution
%			\item Depth superresolution
%			\begin{itemize}
%				\item Non-guided
%				\begin{itemize}
%					\item Learning based
%						\begin{itemize}
%							\item Mac Aodha et al. '12
%							\item etc.
%						\end{itemize}
%					\item Other
%					\begin{itemize}
%						\item Hornacek et al.: Depth Super Resolution by Rigid Body Self-Similarity in 3D, CVPR '13
%						\item etc.
%					\end{itemize}
%				\end{itemize}
%				
%				\item Guided
%				\begin{itemize}
%					\item Joint Bilateral
%					\begin{itemize}
%						\item Yang '07
%						\item etc.
%					\end{itemize}
%					\item MRF based
%					\begin{itemize}
%						\item Diebel and Thrun
%						\item Schuon et al.
%						\item Park et al. '11
%						\item Choi et al. '12
%						\item Shen and Cheung CVPR '13 (Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras)
%						\item etc.
%					\end{itemize}
%					
%					\item Spatio-temporal filtering
%					
%				\end{itemize}
%			\end{itemize}
%		\end{itemize}
%		
%		\item Commercial 3D scanners
%		\begin{itemize}
%			\item MVS based: Photosynth, Adobe, etc.
%			\item Laser scanner systems (industrial)
%			\item Kinect based: Scanec, Reva, ReconstructMe, ...
%		\end{itemize}
%		
%		\item Seen/used calibration references
%		\item Subdivision references
%		\item Efficient raycasting references
%		\item (+ Categorize all the ref e-mails from Gabriel, Oisin, Reading groups, etc.)
%	
%	\end{enumerate}

\chapter{System design} 
\label{chp:my_system}

\par The targeted system attempts to solve the problem of 3D reconstruction of fine surface details in static scenes. We have seen various examples of sparse 3D reconstruction methods using the achievements of multi-view stereo, structure from motion and SLAM techniques in \secref{sec:SFM}. In recent years, a large leap has been made in dense 3D reconstruction by methods in \secref{sec:kinfu}. By designing this system we attempted to address inaccuracies produced by the above mentioned methods to refine high frequency details in small segments of the reconstructed models. \\

\par The system designed acknowledges the power of SLAM performed on the dense model incorporated in the areas of RGB-D based methods. We also recognize the power of other research areas, where inter-sensory information is used to enhance output quality. We hypothesize that there is information collected by the capturing system but is discarded too early in the reconstruction pipeline of a standard RGB-D system. We looked for methods that enable this information to be reintegrated into the results of the reconstruction at later processing steps. The solution was designed based on the alleged potential of iterative, cost-volume based joint bilateral filtering with sub-pixel accuracy in \citep{cvpr-07-qingxiong-yang}. \\

\begin{figure}[h!]\centering
    \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/sys_overview.png}
    \caption{System overview}
    \label{fig:sys_overview}
\end{figure}

\par In \figref{fig:sys_overview} the overview of the designed solution is outlined. The main steps of the targeted system are as follows: (1) RGB-D acquisition and pre-processing, (2) 3D reconstruction, (3) new input acquisition and processing, (4) detail up-sampling and (5) mesh enhancement. Step (3) involves pose estimation of the new input. Ideally, this step enables high resolution RGB images to convey more information about details of the reconstruction. The pose estimation of a captured, high-resolution RGB image can be solved using methods in the area of multi-view stereo, for example bundle adjustment. This step was simulated in our project by the medium-resolution RGB images captured by the Kinect camera, where the pose estimation was performed by the Kinect Fusion algorithm. We thereby show, that there is information in the original input feed that can be used to improve on the output of the 3D reconstruction system. Substantial work was performed to enable mesh enhancement in step (5). However, some problems originating from the fact, that a re-sampling occurs when a 2D projection is created from the estimated pose could not be overcome in the frames of this project. Knowledge from the domain of geometry processing is to be involved to successfully merge the details rediscovered by our solution into the 3D reconstruction.

\par The system outlined above enables iterative refinement of the reconstructed model by acquisition of new input containing new information about the desired details.

\section{Assumptions}
\label{sec:assumptions}

\par When designing a system solving a specific problem, it is important to firmly decide what assumptions are made about the input, what problems are targeted to be solved and what problems are out of the scope of the planned method. We target to design a system, that reconstructs fine surface details of static scenes with mostly Lambertian surfaces allowing offline post-processing. \\

\par The fine surface details of a static scene are to be reconstructed. The problems arising when dealing with dynamic scenes are out of scope for this solution. Rigidity is a common assumption in the field of 3D reconstruction both for multi-view stereo and structured lighting methods. An essential building block of our concept is the possibility to later acquire new data about missing details and integrate it into the reconstruction. Since pose estimation of latter acquisitions is a substantial problem on its own, the assumption about static scenes seems reasonable to make. \\

\par Surfaces with close-to Lambertian reflectivity are considered. During experiments refractive and reflective surfaces appeared in the scene, but the main focus of the system development addresses problems emerging around fine details on mostly Lambertian surfaces. \\

\par The solution will perform post-processing offline. The planned key contribution of the work is to acquire surface details at a finer level than current state-of-the art solutions are capable of. As generally in all fields of computer science, a trade-off between quality and speed has to be made. Thus, the planned solutions operate using offline processing and refinement steps. Precedence in similar projects of using offline processing as capabilities of cloud computing has been observed in applications as Adobe Catch \citep{AdobeCatch} or Microsoft Photosynth \citep{Photosynth}. State-of-the art large-scale RGB-D reconstruction algorithms also may operate off-line, as \citep{Zhou:2013}. \\

\section{3D reconstruction}

The task of accurate real-time mapping of complex and
arbitrary indoor scenes in variable lighting conditions was claimed to have been solved by \citep{Newcombe11}. They list using only a
moving low-cost depth camera and commodity graphics hardware to their contributions as well. There have been made many improvements to the method since it's initial publication detailed in \secref{sec:kinfu}. We base the hypothesis of our work on the claim, that RGB-D reconstruction's attention to the complexity of the scenes can be developed further. Therefore, during the design of the system this algorithm was treated as baseline, its results were reproduced, carefully inspected and an attempt was made to improve upon them. During the extensive research performed to assess the capabilities of derivatives of this algorithm to reconstruct the targeted fine details we did rarely come across projects giving reason to attempt to reproduce their results instead, and treat them as baseline. However, in the future point-based fusion concepts introduced by \citep{keller13realtime} is worth taking into account to improve the starting point of our algorithm.

\begin{figure}[h!]\centering
	\begin{minipage}[b]{0.33\linewidth}
		\includegraphics[width=\textwidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/kinectfusion_example.png}
		\caption{Image from \citep{KinectFuSDKExample}}
		\label{fig:kinectsdk_example}
	\end{minipage}
	\begin{minipage}[b]{0.33\linewidth}
		\includegraphics[width=\textwidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/kinfu_baseline00.png}
		\caption{Mesh reconstruction of scene recorded by us. Reconstruction using PCL's Kinfu with our modifications}
		\label{fig:kinfu_baseline}
	\end{minipage}
	\begin{minipage}[b]{0.33\linewidth}
		\includegraphics[width=\textwidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/lefloch_largescale_cropped.png}
		\caption{\citep{keller13realtime}}
		\label{fig:lefloch}
	\end{minipage}
\end{figure}



\subsection{System input}
\par The system design applied in RGB-D SLAM 3D reconstruction methods aligns well to our initial concept of deign of solution. This is true both in terms of processing pipeline and hardware and software tools applied. The scene acquisition happens using sensors providing both chromatic and distance information. It is a common assumption to use a low-cost, usually hand-held device with integrated sensors as the Microsoft Kinect or ASUS Xtion Pro. A calibrated system of a time-of-flight camera and an RGB camera is another usual system setup. We used a calibrated Microsoft Kinect device. \\

\par An RGB-D SLAM 3D reconstruction system operates with at least two input streams. A data stream containing chromatic information about the observed scene, usually encoded in 8-bits per channel, and another information source conveying information about the spatial relations of the scene encoded in some form of distance information.
\par The Microsoft Kinect is capable of conveying three channels of chromatic data encoded in a 24-bit data-stream. The most important available resolution settings with the used OpenNI Middleware are 640 x 480 pixels at \mytilde 30 frames per second and 1280 x 1024 pixels at \mytilde 15 frames per second. The quality of the RGB image seems to suffer from the problems Bayer filtering type methods introduce, as it can be seen in \figref{fig:kinect_rgb_zoom}. This introduces slight corner detection problems at calibration time, and more significant problems when applying the up-sampling algorithm as discussed in the limitations section \secref{sec:limitations}. 
\par The depth stream is available in the resolution 640 x 480 at \mytilde 30 frames per second. The critical analysis of the quality of the depth map has been researched in many projects, amongst others by modelling the noise of the sensor \citep{NguyenIL12}, or applying temporal filtering \citep{RGBZcamera}. It is considered out of the scope of the current project to pre-process the depth stream before applying it in the system pipeline. \\

\par To establish working volume boundaries and area of focus, the operating range and optimal scanning distance of the used depth acquisition device has to be known. According to \citep{Kinect_ms} the Kinect depth sensor has a minimum range of 800 mm and a maximum of 4 m. The Kinect for Windows Hardware can work in Near Mode providing a range of 500 mm to 3 m. By sampling the depth maps acquired during our data collection process it can be stated, that these characteristics can be achieved using the OpenNI Middleware as well, depths around 500 mm have been observed. 
\par In future work, the \citep{Kinect_nyko_zoom} might provide possibility to zoom in further on the scene and acquire more accurate depth data about fine details, meanwhile the distortion effect of the additional lens has to be evaluated as well.

\subsection{Processing algorithm}
\par An open-source implementation of the classical Kinect Fusion algorithm was employed to retrieve a smooth 3D reconstruction from the data acquired. The \citep{PCL} has implementation both for the original small to mid-scale algorithm named "Kinfu" and an implementation capable of handling scenes with larger extent called "Kinfu Large Scale". Due to the targeted proof-of-concept nature of our project the computationally slightly less intensive Kinfu implementation was used as starting point. The sub-project was recompiled to a separate, modifiable library allowing slight customisations to the algorithm (project "MyKinfuTracker" in the source code attachment). Algorithmic changes or implementation of later publications detailed in \secref{sec:kinfu} incorporating improvements to the concept are considered out of scope for this project.\\

\par The outline of PCL's Kinfu algorithm is as follows: All depth data streamed from a Kinect sensor is merged into a global implicit surface model stored in a fixed resolution voxel grid in the GPU's memory. In each voxel grid cell the distance to the closest surface is stored as a truncated signed distance function (TSDF). Subject to convention the two signs represent the inside respectively outside of the assumed non-degenerate mesh. 
\par Upon arrival of new depth data a 3D point vertex and normal map is generated using camera the intrinsics. Correspondence to the stored mesh represented by the stored TSDF is calculated using ray-casting and the point cloud represented by the vertex map is used to estimate the new camera pose with respect to the current model. This is performed using a coarse-to-fine iterative closest point algorithm. 
\par The point cloud is then regenerated using the new camera pose and the appropriate cells of the voxel grid are updated. A new vertex and normal map is generated to prepare for the next arriving frame. The update method has large impact on our project goals. In \secref{sec:3d_sys_output} the role is discussed further with other parameters of the algorithm. \\

\par By recompiling PCL's Kinfu solution to a stand-alone library we prepared the implementation for future modifications. This allowed us to experiment with the different parameters detailed in the next section. These were hard coded into the solution in order to enable CUDA code to perform optimally. The most important modification besides change of parameters was to turn off the initial bilateral filtering of the input depth to enable higher attention to detail. The effectivity of this modification was suppressed by the averaging nature of the TSDF update method. Since the rest of the implementation assumed filtered depth maps with no missing information, a bilateral filtering step was reintroduced, that fills only pixels with missing data. A more accurate solution would require missing data to be disregarded at later stages of the pipeline.

\subsection{Configuration and system output}
\label{sec:3d_sys_output}

The results of the reconstruction stored in the voxel grid as a TSDF volume can be exported to several output formats. A GPU implementation of the classical marching cubes algorithm \citep{Lorensen:1987} exports the mesh into a point cloud. A triangle mesh can also be exported using the same method and connecting neighbouring grid cells to faces. The storage format is greedy storing all three vertices for every face. This introduced algorithmic challenges when performing subdivision on the mesh at later stages of the pipeline. The results can also be visualized using ray-tracing and Phong shading. We used the triangle mesh as output of this system in the pipeline.

\par Configuration parameters can be used to tweak the system to yield more optimal results, a reconstruction with higher attention to detail. The most important parameters of the Kinect Fusion algorithm are voxel grid resolution, working volume size, truncation distance, maximum movement threshold and weight of new information.

\par The resolution of the voxel grid and working volume size has the highest impact on the accuracy of fine details in the reconstruction. The parameters used during reconstruction resulted in a spatial resolution of $3 m / 512$ or $3 m / 640$ meaning an average accuracy of 5.85 mm respectively 4.68 mm. This implied a $3m^3$ working volume represented by $512^3$ respectively $640^3$ voxels. Higher values were bounded by hardware constraints (GPU memory size). The quality of the final output of our system was compared relatively to the output of the 3D reconstruction in the \secref{sec:discussion_results}.

\par The truncation distance has an impact on the accuracy of cavities and narrow details. The importance of this parameter is relevant when performing the raycasting steps of the algorithm. Since our solution does not need real-time performance this value could be increased to result in results with higher accuracy. Thus the output quality is not limited by this parameter, but by the spatial resolution of the voxel grid.

\par The maximum movement threshold is relevant to the pose estimation step of the algorithm. This parameter ensures, that the pose estimation algorithm did find a good match between the model and the new frame. By increasing the tolerance for movement between frames, we would allow less accurate pose estimations and faster camera movement. By decreasing the value more information would get discarded.

\par As stated earlier, the TSDF update method is an important parameter in terms of the reconstruction quality. In the given implementation, the information conveyed by new measurements are averaged giving each measurement equal weight. On the \citep{KinectFuSDKExample} page they say, that the weight parameter {\it "...controls the temporal averaging of data into the reconstruction volume – increasing makes the system a higher detailed reconstruction, but one which takes longer to average and does not adapt to change. Decreasing it makes the volume respond faster to change in the depth (e.g. objects moving), but is noisier overall."}. The equal weighting of all measurements has a large impact on the output introducing a strong smoothing effect discarding fine surface details and giving place to the hypothesis of our project.

\section{Depth super-resolution}

\par The base hypothesis of our project states, that there is information recorded by the sensors that is discarded early in the pipeline. We claim, that by identifying and reintegrating this information at later stages, it is possible to improve the reconstruction of fine surface details in comparison to existing 3D reconstruction methods. 

\par In \secref{sec:super_resolution} many image-, depth- and joint super-resolution methods were overviewed. The claimed power of the method published by \citep{cvpr-07-qingxiong-yang} served as basis for the main hypothesis of the project. In their works they demonstrate successful ten-fold up-sampling of noisy, low resolution time-of-flight depth images with the help of registered RGB images. Later works often refer to \citep{cvpr-07-qingxiong-yang} as super-resolution by joint bilateral filtering. However, the original method applies joint bilateral filtering in an iterative fashion on a cost-volume originally necessary for the stereo-matching problem targeted. The main hypothesis of cross-bilateral filtering says that neighbouring pixels that are spatially closer and similar in [colour] appearance are more reliable and thus to be given more weight when estimating a new value for the center pixel of the filtering kernel.
\par The two main contributions of their project emphasized in our solution is this neglected iterative nature of their method and the implementation of sub-pixel accuracy to the depth search range and its resolution. An overview of their algorithm can be seen in \figref{fig:yang_overview}. The full algorithm was reimplemented during the course of the project. Given its computational intensity, a full GPU implementation was developed further described in \secref{sec:implementation_details}

\begin{figure}[h!]\centering
    \includegraphics[width=0.4\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/yang_overview.png}
    \caption{System overview of \citep{cvpr-07-qingxiong-yang}}
    \label{fig:yang_overview}
\end{figure}

\subsection{System input}

\par The system takes two registered images as input. One of the images is the depth information targeted for up-sampling. This image is supposed not to contain details to the desired scale. The second input is a colour image conveying the extra information used for enhancement. By applying the method described, the depth image can be up-sampled to the same resolution as the colour image. The exact registration of the images has proven itself to be of high importance. The system is also sensitive to several parameters as input, described in the next subsections.

\subsection{Processing algorithm}
\par The depth image is up-sampled to the resolution of the RGB image, and
serves as the initial hypothesis of the depth information. In our implementation a nearest neighbour strategy is applied to reduce the number of possible sources of artifacts. 
\par Afterwards, an iterative refinement process is started. First a cost volume is built based on the current depth estimate. For every pixel, every depth hypothesis within a relatively narrow search range is compared to the current hypothesis. The cost of the investigated depth value depends on the distance from the current hypothesis subject to a cost function. The algorithm uses a truncated quadratic cost function to enable large depth discontinuities, meanwhile ensuring a smooth gradient search.
\par Then joint bilateral filtering is applied to the cost volume to handle artifacts in the proximity of large gradients. The necessity of this step is supported by an undesired fattening effect around edges. By applying bilateral filtering the compatibility of the new depth value to its neighbourhood is investigated. This compatibility is modulated by the spatial distance between the neighbouring pixel subject to a Gaussian fall-off, and the distance in appearance also subject to a Gaussian fall-off with different variance.
\par After filtering the cheapest new depth value is selected by a winner-take-all strategy. Then the cost of the two neighbouring hypotheses are used to perform a sub-pixel estimation procedure to enhance the resolution determined by the search step during the construction of the cost volumes.

\par The stop condition is not described in the original paper, but the published datasets allow us to conclude that around 130 iterations were performed on depth data in the 0..255 range. Since our depth images usually lie in a larger range of 0..3000, a minimum-average-difference stop condition was tested in the implementations. Finally we decided to fix the allowed change of a depth value based on the assumption, that we can assume a maximum change in mm-s given the targeted scene.

\subsection{Configuration and system output}

\par The algorithm is moderately sensitive to several parameters that need to be adapted to the domains of the current problem. The highest sensitivity is introduced through the joint bilateral filtering. 

\par The variance of the two Gaussian fall-offs are to be set. The current solution operates with fixed variances, and the adaptivity to the characteristics of the input data is accomplished through the fact, that the appearance information is taken into account during the filtering. High spatial variance can be interpreted as a larger smoothing filter granting influence to more distant neighbours in the filter kernel. Low spatial variance dampens the smoothing effect to the cost of the effectivity of the filtering. A future improvement to our algorithm is to automatically calculate the maximum necessary filter kernel size from this variance (an often seen heuristic is to set it to two times the standard deviation), currently it's set separately.
\par The variance of the Gaussian responsible for assigning weights to neighbours based on appearance information is also an input parameter. A high variance can be interpreted as being more allowing regarding the appearance of the neighbours. This is favourable if the quality of the colour input is low. Low variance means being very restrictive and assuming good input quality where object similarity is well captured by the colour information. The original algorithm applies an equally weighted gray scale conversion. In future work the effect of not discarding hue information should be investigated. Given the low quality of the high resolution RGB images returned by the Kinect's camera, the gray scale conversion was a necessary decision.

\par Bilateral filtering methods usually allow several iterations to be performed to handle data with higher noise levels. The effect of this can be translated to a larger kernel size and thus only one iteration of filtering was performed on each slice of the cost volume. This decision is also supported by the fact, that the whole algorithm relies on an iterative concept.

\par The iterative up-sampling algorithm relies on parameters determining the truncation of the cost function, the search range and its resolution and the fixed maximum iteration count implementing the stop condition. A lower truncation level of the quadratic cost function is desired to preserve large depth discontinuities in the scenes. The search range is quite statically determined by the depth domain of the scene, and is recalculated at each iteration to speed up computations. The range is set to contain the current depth spectrum padded by the truncation value of the cost function. 

\par The search step parameter determines the maximum possible accuracy of the method without the sub-pixel refinement. It was set to 1 mm in during experiments. A value inherited from the fact, that the input depth data is provided encoded as 16 bit integers with mm precision. The calculations in the pipeline are performed on float data, and due to the sub-pixel accuracy calculations, have not too large impact. The interpretation of the search step is, that a quadratic function is fitted using three samples spaced by the search step. This gives a hard to interpret notion of accuracy.

\par In the original paper there is no mention of representation errors or limitation of the sup-pixel accuracy estimation due to close to zero divisions. In our implementation a salt-pepper like noise was observed close to very large edges. Therefore a change limitation was introduced to bound the output of the sub-pixel estimation step to the range $\pm 1 \cdot search\_step$.

\par The output of the algorithm is demonstrated in the validation chapter \chpref{chp:validation}. The validity of the reimplementation is also tested on the datasets attached to the original publication.
        
\section{Combined system pipeline}
\label{sec:combined}

\par The designed system accomplishes the targeted high-resolution 3D reconstruction by combining the above outlined two systems, smooth 3D reconstruction and depth up-sampling. The overview of the constructed system is shown in \figref{fig:sys_overview}. A very important concept in the designed solution is the possibility of including new data to enhance parts of the reconstruction previously not sufficiently observed. This allows for a freedom to combine sensors and their characteristics. This theoretical design feature was tested by using the existing acquired data to check, whether there is unused information not picked up by the initial reconstruction. The pose estimation of the Kinect Fusion algorithm is used to render a new depth image of the reconstructed model from a simulated virtual viewpoint. Since the pose is known, the aligned RGB data can be used to up-sample the depth from the virtual viewpoint.

\subsection{Pre-processing the input}

\par To ensure high quality alignment required by the later parts of the pipeline a extensive work was performed to gather information about the extrinsic and intrinsic parameters of the recording device. 10-component intrinsic parameters for both lenses were calculated alongside with the estimation of the extrinsics connecting the two cameras. Further details are discussed in \secref{sec:calibration}.

\par The estimated camera parameters are used to remove lens distortion from both input streams and to reduce the two coordinate systems to one by mapping the viewpoint of the depth camera to the coordinate space of the RGB camera. By doing this before the streams are provided to the smooth reconstruction algorithm a compromise is made. The mapping calculations introduce estimation errors, loss of depth data by occlusion in the new viewpoint and representation problems due to the non-rectangular output of the distortion removal algorithms. The gain is to be able to use the aligned input feeds for further, quality enhancing pre-processing algorithms, for example joint-bilateral filtering. The strong smoothing effect of the 3D reconstruction algorithm applied takes away the edge of this gain, in future work the viewpoint mapping is to be moved to after the smooth reconstruction step.

\subsection{Rendering a virtual viewpoint}

\par After the smooth 3D reconstruction, any virtual viewpoint of the system has to be possible to render. Therefore, the results of the reconstruction have to be stored. Furthermore the projection parameters of the virtual viewpoint have to match the characteristics of the capturing device that the newly acquired data was recorded with. The actual rendering of the model representation has to be performed, and information about the sampled parts of the mesh has to be collected.

\par The calculation of the actual position of the virtual viewpoint is detailed in the future work section \secref{sec:future_work}. Bundle adjustment implemented in \citep{SnavelySS06} or \citep{vsfm} can be used to align the new RGB data with the available snapshots from the Kinect stream. The new sparse and stored dense reconstruction model can than be aligned, so that the estimated new camera pose can be transformed between the two coordinate systems.

\par To  store the results of the smooth reconstruction in a loss-less fashion the data structure of the Kinfu implementation is to know. The 3D voxel grid is mapped to a 2D storage array in the GPU memory. In order to be able to perform a large variety of sampling operations on the results the original grid was saved to disk using existing tools. The read-back to GPU memory had to be implemented. The exact storage of the TSDF volume enables ray-casting to be performed to render the virtual viewpoint. Using the marching cubes algorithm the theoretical accuracy of this representation is approximated. However by storing triangle meshes in PLY format a wider range of existing rendering tools became available and calculation times were heavily reduced. This practical separation of the pipeline can be reversed in future work. The loss of accuracy was attempted to be counter-weighted by the employment of different mesh subdivision algorithms. The sub-optimal data structure of the triangle mesh is not compatible with the available existing subdivision implementations. Since this algorithmic step will prove unnecessary in future work, further attention was not given to the problem arising through an earlier approximation step.

\par To render the virtual viewpoint of the reconstructed model three different approaches were developed. PCL's octree implementation allowed the quick development of a ray-casting algorithm. The execution was performed on the CPU very similarly to the Kinfu GPU algorithm, thus the projection parameters involving the used camera intrinsics were closely controlled. The information about visible vertices and faces is easily retrievable using this method as well. This information is needed by the mesh enhancement step, where the modified primitives have to be merged with the unmodified primitives of the mesh. Unfortunately our meshes consisted of 1.5 - 2.5 million vertices which made the CPU implementation too slow despite the effective octree point cloud representation. Problems arised in form of the hand-tuning of the maximum octree grid resolution and the need to account for larger faces occluding rays but not represented by the octree structure.

\par GPU based renderers of triangle meshes are widely known. PCL relies on the library called \citep{vtk}. Unfortunately in its newest version the possibility to specify all elements of the projection matrix independently is not enabled. Therefore, after many attempts to use the offered interface to accomplish the incorporation of the exact intrinsics into the rendering system a third method had to be discovered. Visibility information is also not retrievable using this method.
\par A GLSL based rendering environment was implemented to ensure full control over the projection parameters, quick rendering of the large number of input triangles, and full information about appearing vertex- and face ids. A FramebufferObject is set as render target, the depth buffer texture is read and rescaled to retrieve the depth map, and unclamped unsigned textures are attached as fragment shader outputs to retrieve information about sampled vertices and faces.

\subsection{Model enhancement}
\par Several strategies were developed to accomplish the substitution of the projected vertices and faces by the up-sampled depth data. It is established, that the re-sampling nature of the problem requires solutions to be imported from the geometry processing domain. Operations like this were considered out of scope for the current project.

\par As results the up-sampled 2D depth maps containing some desired and other unwanted high-frequency details are rendered as 3D point clouds and triangle meshes using the naive parametrisation provided by the pixel grid of the 2D image. Further details in \secref{sec:results}.

\section{Calibration}
\label{sec:calibration}

\par In the previous sections it was established that correct alignment of the input sources shown in \figref{fig:input} has to be executed to enable the up-sampling algorithm to perform. In \figref{fig:calib_built_in} the built-in calibration is shown to be not accurate enough. This calibration is provided by the OpenNI middleware and is stated to be read from the hardware chip of the recording device. By paying attention to narrow non-gray areas (IR occlusions) in the image one can observe the misalignment of the contours of the images.

% Input rgb and depth
\begin{figure}[h!]\centering
    \begin{minipage}[b]{0.49\linewidth}
        \includegraphics[width=\textwidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/SuperRes-NI-1-5/build/out/imgs_20130725_1809/img8_00000001.png}
        \caption{Input RGB from Microsoft Kinect camera}
    \end{minipage}
    \begin{minipage}[b]{0.49\linewidth}
        \includegraphics[width=\textwidth]{/media/Storage/Dropbox/UCL/project/results/presentation_300713/dep8_00000001.png}
		\caption{Input depth from Microsoft Kinect camera}
    \end{minipage}
    \label{fig:input}
\end{figure}

% Simple alignment
\begin{figure}[h!]\centering
	\begin{minipage}[b]{0.49\linewidth}
		\includegraphics[width=\textwidth]{/media/Storage/Dropbox/UCL/project/results/presentation_300713/dep16AndRgb_00000001.png}
		\caption{No alignment}
	\end{minipage}
	\begin{minipage}[b]{0.49\linewidth}
		\includegraphics[width=\textwidth]{/media/Storage/Dropbox/UCL/project/results/presentation_300713/dep16AndRgb_00000000.png}
		\caption{"Built-in alignment" from OpenNI Middleware}
	\end{minipage}
	\label{fig:calib_built_int}
\end{figure}

\par The calibration procedure was performed between "raw" images captured by the IR camera and the colour images captured by the RGB camera. Approximately 70 images of both sources were annotated for user-supported corner detection. The calibration images were taken with the structured light projector covered, since that introduces intolerable noise to the corner detection performed on the IR images. Therefore a space with plenty ambient IR lighting (sun light) had to be used for the calibration. The calibration images were taken at several distances to ensure coverage of the whole working volume. Different perpendicular and skew angles were recorded covering the whole, and several sub-parts of the view area. A real-time visual feedback loop was developed to ensure that the full checker-board pattern is visible on both images. Pixel re-projection errors below 0.5 pixels were targeted to reach "sub-pixel accuracy". An A3 format checker-board was the smallest size yielding good enough results (evaluated qualitatively). For future work even larger formats are suggested. The printing method should stretch the paper as little as possible, and the calibration pattern should be attached to a perfectly flat surface with extra caution to  perfect alignment to the surface.

\par Extra attention was paid to the information available about the transformation algorithm that the Primesense sensor uses to convert the IR data to the provided depth images. According to several blog sources the width of the width of the output depth image is a result of 9x9 kernel up-sampling of the raw IR image. Therefore the calibration results were tested using this \mytilde 4.5 pixel offset. The accuracy of the reached calibration did not allow us to determine whether this makes sense. Some errors were corrected, some others were introduced as it can be seen in \ref{fig:calib_ir_depth}


\begin{figure}[h!]\centering
    \begin{minipage}[b]{0.49\linewidth} \label{fig:ir}
        \includegraphics[width=\textwidth]{/media/Storage/Dropbox/UCL/project/results/presentation_300713/irAndDep8_00000001.png}
        \caption{Default IR and depth overlay. A suspected offset of 4 pixels was examined.}
    \end{minipage}
    \begin{minipage}[b]{0.49\linewidth}
        \includegraphics[width=\textwidth]{/media/Storage/Dropbox/UCL/project/results/presentation_300713/offsIrAndDep8_00000001.png}
        \caption{IR offset by radius of supposed convolution kernel. Some edges improve, some worsen.}
    \end{minipage}
    \label{fig:calib_ir_depth}
\end{figure}

\par Using the \citep{calibration_bouguet} the intrinsic parameters were estimated of both lenses. The calibration includes parameters skew ($\gamma$), radial distortion up to the third-order ($k_1$,$k_2$,$k_3$) and tangential distortion up to second-order ($t_1$,$t_2$). The extrinsic relationship between the two cameras was established as well. The \mytilde 30 mm translation between the two cameras seemed to be plausible results. Skew was hardly detected in any of the cameras, but the third-order radial distortion was \mytilde 0.5 for both cameras. The final re-projection error for the RGB camera was in the 0.1 pixel magnitude, the IR intrinsic calibration resulted in re-projection errors of 0.3 pixels. Since the re-projection is subject to the errors of the corner detection as well, these results were considered successful. The final alignment after viewpoint mapping and lens distortion removal can be seen in \figref{fig:final_calib}.

\begin{figure}[h!]\centering
        \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/results/blendedUndistortedUC3x2.png}
        \caption{Mapping from 640x480 depth to 1280x1024 RGB after calibration and distortion removal}
        \label{fig:final_calib}
\end{figure}

\par The mapping algorithm implemented in the toolbox runs in the MATLAB framework. The mapping of a 640 x 480 depth image to the 1280 x 960 RGB image took around 1 minute to calculate for each frame. Therefore CUDA code from \citep{kinect_lua} was rewritten to perform the mapping and undistortion with speeds of the 1 ms magnitude for a frame.

\section{Implementation details}
\label{sec:implementation_details}

\par Besides a single core C++ implementation, a less optimised, GPU based parallel implementation was created using the NVIDIA's Thrust library. Since both of these kept the run times for a single frame in the hour magnitude at the resolution of 1280 x 1024, a more optimised CUDA implementation was also developed. This implementation brought down run-times to 8-15 minutes on 

\subsection{Drivers}
\subsection{Third-party libraries}
simultaneous rgb and IR problems
NVIDIA bilateralfiltering sample
PCL
 pcl bilateral upsampling
 \citep{DCBGridStereo}
 
%https://github.com/jtbates/lua-kinect/blob/master/depth_to_point_cloud_par.cu
 OpenCV
 Eigen
 Thrust
 OpenNI, freeglut
 
Renderings made using \citep{Meshlab}.

%%% CHAPTER 5 %%%
\chapter{Experiments}
\label{chp:validation}

We targeted to build a system, that proves the hypothesis, that there is information discarded early in the 3D reconstruction pipeline and that it can be reintroduced later. The developed system also enables additional, possibly more detailed input to be captured and added the reconstruction later. The outline of the algorithm to be validated is repeated in \figref{fig:sys_overview2}.

\begin{figure}[h!]\centering
    \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/sys_overview.png}
    \caption{System overview}
    \label{fig:sys_overview2}
\end{figure}

The distortion removal and alignment of the input data was validated in \figref{fig:final_calib} and \figref{fig:limit_kinfu_pose}. The smooth reconstruction was tested extensively during mesh subdivision experiments. The successful rendering from a virtual point of view is demonstrated in \figref{fig:pose_138_in_out} on the left side. The reimplementation of the discussed depth super-resolution algorithm as one method of reintegrating discarded information is checked in \figref{fig:sofa}, \figref{fig:umbrella}, \figref{fig:chair}, and \figref{fig:pink}. The final output of the system is shown in \secref{sec:results}.

%%% SECTION results %%%
\section{System output}
\label{sec:results}

\par Several recordings have been made scanning a detail-rich desktop environment. The scenes contain diffuse surfaces, textured surfaces, transparent surfaces and reflective surfaces as well. As stated among assumptions, our primary focus was on close to Lambertian surfaces. 
\par The output of our system are up-sampled depth images with possible resolutions up to the input resolution of the RGB images. To ensure minimum loss of accuracy and portability, the output is saved as ASCII float matrix with extension the extension PFM. These images are then rendered in 3D using the calibrated camera intrinsics. The rendering outputs are provided in the format PLY. Both point clouds and naive triangle meshes are rendered. The naive triangle rendering supposes, that the depth image coordinate grid is a uniform parametrisation and connects neighbouring triplets to non-overlapping triangles. Some rendering artifacts due to large pixel discontinuities are considered side effects of the visualisation, and originate in non-perfect parameter settings of our method. Textured point clouds and triangle meshes are also rendered to show the origin of estimation errors. Some of the results of the system can be seen in \figref{fig:keyboard_192_final}, \figref{fig:keyboard2_192_final}, \figref{fig:pose_138_in_out}, and \figref{fig:pose14}. For some of the outputs' rendered meshes, please see attachment. The results are discussed in \chpref{chp:discussion}.

\begin{figure}[h!]\centering
    \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/keyboard_192_final.png}
    \caption{Close up rendering of keyboard upsampled from skew view angle. Output textured point cloud and output rendered naive triangle mesh. Frame 192.}
    \label{fig:keyboard_192_final}
\end{figure}

\begin{figure}[h!]\centering
    \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/keyboard2_192_final.png}
    \caption{Close up rendering of different keyboard upsampled from closer to normal view angle. Output textured point cloud and output rendered naive triangle mesh. Frame 192.}
    \label{fig:keyboard2_192_final}
\end{figure}

\begin{figure}[h!]\centering
    \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/138_in_out.png}
    \caption{Close up rendering of keyboard and graphics card. Left: rendering from virtual point view; Right: output rendered naive triangle mesh. Frame 138.}
    \label{fig:pose_138_in_out}
\end{figure}

\begin{figure}[h!]\centering
    \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/pose14.png}
    \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/pose14_2.png}
    \caption{Effect of non-geometrical edges. Output textured point cloud and output rendered naive triangle mesh. Frame 14.}
    \label{fig:pose14}
\end{figure}

%%% SECTION %%%
\section{Iterative joint bilateral up-sampling with sub-pixel accuracy}
\label{sec:yang}

\par The second part of the pipeline consists of the re-implementation of the work published by \citep{cvpr-07-qingxiong-yang}. To explore the limits of the method and to verify the correctness of the implementation several experiments were executed. One of the methods biggest limitations are depth changes introduced by non-geometric edges in the RGB images. To test this, the an aligned RGB-D input of a checker-board pattern was served as input to the CUDA implementation of the algorithm. The result can be seen in \figref{fig:yang_checkerboard}. 
\par When designing the system an assumption was made that this depth up-sampling method reintroduces information to the scene from the RGB image. During initial tests, this assumption seemed to change characteristics. The current assumption is that the method is only capable of correcting depth values in the range of the observed depths. Not even the sub-pixel accuracy element ensures that new, unseen depths are introduced. Therefore a synthetic test was set up to understand the capabilities of the algorithm. The same checker-board RGB image responsible for \figref{fig:yang_checkerboard} was paired with a depth image of constant depth. As suspected, the result yielded an unchanged depth map, no new depth values were introduced outside the observed range of depth values.

\begin{figure}[h!]\centering 
        \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/checkerboard.png}        
        \caption{Example run of implementation on checker-board scene, results without and with texture after 50 iterations.}
        \label{fig:yang_checkerboard}
\end{figure}

To make sure, that the above experiences are not due to a faulty implementation of the method, some the available inputs from the original dataset were processed, all with success. \figref{fig:sofa}, \figref{fig:umbrella}, \figref{fig:chair}, \figref{fig:pink}

\begin{figure}[h!]\centering
	\begin{minipage}[b]{0.49\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/sofa_gt.png}
	\end{minipage}
	\begin{minipage}[b]{0.49\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/results/sofa00.png}
	\end{minipage}
	\caption{"Sofa" output by original paper (left) and by our implementation(right). Some artifacts due to untuned range variance.}
	\label{fig:sofa}
\end{figure}

\begin{figure}[h!]\centering
	\begin{minipage}[b]{0.49\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/umbrella_gt.png}
	\end{minipage}
	\begin{minipage}[b]{0.49\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/results/umbrella00.png}
	\end{minipage}
	\caption{"Umbrella" output by original paper (left) and by our implementation (right).}
	\label{fig:umbrella}
\end{figure}

\begin{figure}[h!]\centering
	\begin{minipage}[b]{0.49\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/chair_gt.png}
	\end{minipage}
	\begin{minipage}[b]{0.49\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/results/chair00.png}
	\end{minipage}
	\caption{"Chair" output by original paper (left) and by our implementation (right).}
	\label{fig:chair}
\end{figure}

\begin{figure}[h!]\centering
	\begin{minipage}[b]{0.49\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/pink_gt.png}
	\end{minipage}
	\begin{minipage}[b]{0.49\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/results/pink00.png}
	\end{minipage}
	\caption{"Pink" output by original paper (left) and by our implementation (right).}
	\label{fig:pink}
\end{figure}


%%% SECTION %%%
\section{Structure from motion} 
\label{sec:sfm}

\par Due to recent achievements of the fields multi-view stereo and structure from motion, the reconstruction capabilities of these methods were tested in the beginning of the project to decide, whether a sparse or dense 3D reconstruction algorithm is to be used. Recordings were created using the video recording function of a rather modern smart phone. The 1080p image stream was down sampled to 960 x 540, and the frame rate of the video was decreased using sub-sampling to 5 FPS. This data was input into both \citep{Photosynth} and \citep{vsfm}, state-of-the art sparse stereo reconstruction algorithms. In general, Microsoft Photosynth performed better with the reconstruction of these scenes. These tasks are otherwise considered to be hard for multi-view stereo due to transparency and the repetitive, feature-less characteristics of the recordings. Results are shown in \figref{fig:acerlit_1}, \figref{fig:acerlit_mesh}, \figref{fig:ob_1} and \figref{fig:ob_mesh}.

% acerlit image
\begin{figure}[h!]\centering
	\begin{minipage}[b]{0.4\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/acer_lit00706.jpg}
		\caption{A frame of video of backside of a monitor.}
		\label{fig:acerlit_1}
	\end{minipage}
	\begin{minipage}[b]{0.49\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/acer_lit01.png}
		\caption{Visual SFM 3D reconstruction of monitor. Almost no sense of 3D has been captured.}
		\label{fig:acerlit_mesh}
	\end{minipage}
\end{figure}

% oil bottle image
\begin{figure}[h!]\centering
	\begin{minipage}[b]{0.49\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/ob00003r2.jpg}
		\caption{A frame of video of oil bottle}
		\label{fig:ob_1}
	\end{minipage}
	\begin{minipage}[b]{0.49\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/ob_mesh00.png}
		\caption{Visual SFM 3D reconstruction of oil bottle. Almost no sense of 3D has been captured.}
		\label{fig:ob_mesh}
	\end{minipage}
\end{figure}



\chapter{Discussion} 
\label{chp:discussion}

\section{Results}
\label{sec:discussion_results}

\par A thorough investigation has been performed in \chpref{chp:experiments} to identify strengths and weaknesses of the designed system. Based on the evaluation we claim to have developed a proof-of-concept system that identifies major challenges arising during the solution of the targeted task. The initial hypothesis of existing discarded data can be regarded confirmed, since on all of the final output images in \secref{sec:results} we can see depth detail of granularity not observed in current 3D reconstruction algorithms. The level up to which the keyboard keys are  distinguishable in \figref{fig:keyboard2_192_final} is encouraging. However there is a very apparent problem of viewpoint dependency of the algorithm. In all resulting figures one can see, that the details are not put into the right depth, i.e. the keyboard keys are not elevated from the smooth surface, but partially sunk in. The problem becomes even more severe, when steep view-angles are observed as in \figref{fig:keyboard_192_final}. The fact, that the 3D depth data is mapped to the RGB image, and not the opposite can be attributed this effect. In future work, the possibilities of determining a decision logic, controlling whether the individual parts of the here shown up-sampled results should be integrated into the mesh. Another, possibly even more effective approach is to investigate, what conditions have to be fulfilled to be able to use the up-sampling algorithm from an ideal, normal directed viewpoint by taking the information conveyed in the new colour image into account through ray-casting it to the smooth mesh surface. Obviously the sparsity of the colour data would have to be one of the challenges to be targeted.

\par The decision about the possibility of rendering a virtual viewpoint and applying a newly acquired RGB image to enhance it's quality is not obvious. Although the calibration, smooth reconstruction and depth up-sampling steps of the pipeline have successfully been validated, the rendering and alignment of the new viewpoint has not. The problems are suspected to rely on two grounds. First, there are too many unknown factors for hold-one-out validation of the pose rendering. The calculation errors introduced by the changes of the intrinsic matrices due to aspect-ratio changing resolution transforms would have to be estimated using a more thorough approach. Second, the pose estimation accuracy of the Kinect Fusion algorithm is shown to be quite limited for the accurate needs of our pipeline. For example in \figref{fig:limit_kinfu_pose} one can see a reasonably well aligned depth and RGB image that come directly from the calibrated Kinect device. However, the virtual depth map and the RGB are misaligned, mostly along the y-axis. Since the aspect-ratio change due to resolution transformations happens along the x-axis, the Kinfu pose estimation is suspected to be the reason to the observed misalignment of the data. Additionally, the event-based, so called "triggered capture" nature of the middleware applied to the two fold difference in stream update speeds (15 FPS vs. 30 FPS) can also contribute to the pose estimation errors.

%
%\subsection{Dense or sparse reconstruction}
%
%\par An easy to operate, hand-held scanning device is used. The developed system will most likely use a smartphone with built in inertial measurement unit, gyroscope, magnetometer and GPS sensor. Alternatively a camera with mounted and calibrated motion and orientation sensors can be used. The performance of structured lighting devices as Microsoft Kinect \cite{Kinect} or Asus Xtion PRO \cite{XtionPro} will also be explored. Eventually a higher resolution device as Leap-motion's gesture recognition solution \cite{LeapMotion} will be evaluated. To ensure interchangeability of the sensors, and to get these systems to cooperate to the highest level possible, a well designed framework needs to be developed. The extensibility of the KinectFusion SDK will be tested once it get's released as part of the Windows 8 SDK as promised \cite{SDKKinectFusion}. Alternatives, as the implementation of a Kinect Fusion like functionality in the Point Cloud Library ("Kinfu") will also be tested \cite{KinFu}.
%\par The commercial accessibility of the used sensor setup has a slightly increased emphasis at this stage of the project. Low cost and out of the box operability would open the possibilities for crowd sourcing in follow-up projects, something that has been targeted by many other research labs as Microsoft (at Techfest 2013) or the University of Washington (Photocity, Pointcraft).
%
%/// The current sensor pose is simultaneously obtained by
%tracking the live depth frame relative to the global model using a
%coarse-to-fine iterative closest point (ICP) algorithm, which uses
%all of the observed depth data available. We demonstrate the advantages
%of tracking against the growing full surface model compared
%with frame-to-frame tracking, obtaining tracking and mapping results
%in constant time within room sized scenes with limited drift
%and high accuracy. We also show both qualitative and quantitative
%results relating to various aspects of our tracking and mapping system.
%Modelling of natural scenes, in real-time with only commodity
%sensor and GPU hardware, promises an exciting step forward
%in augmented reality (AR), in particular, it allows dense surfaces to
%be reconstructed in real-time, with a level of detail and robustness
%beyond any solution yet presented using passive computer vision.

\section{Limitations} 
\label{sec:limitations}

\par Based on the demonstrated results in \chpref{chp:validation} the strengths and weaknesses of the employed sub-systems can be assessed. The most important sub-systems evaluated are calibration, Kinect Fusion and depth up-sampling.
\par The calibration toolbox applied in this project is not the most advanced one. State of the art calibration treats range sensors differently by building a noise model before optimisation of the intrinsic parameters \citep{calibration_herrera}. The problem of IR to depth image misalignment is also overcome when estimating calibration using the depth images. More of these have to be acquired though, to provide enough data to the noise model estimation.
\par The Kinect Fusion implementation applied in this project provides a powerful method for smooth 3D reconstruction. It handles static mid-scale static scenes well, and degrades over dynamic scenes gracefully. State-of-the art improvements have targeted the limitations of the system used in this solution, such as limited work volume \citep{Whelan13iros}, averaging over scale space \citep{Fuhrmann:2011} and memory footprint \citep{keller13realtime}. The limitation faced in this project with the highest impact is the pose estimation accuracy. The applied ICP algorithm and it's static movement threshold did not yield suitable quality of the pose estimates. During reconstruction, sequences of frames were observed with larger displacement or more intensive viewangle changes where the pose estimation was a big problem. Around more static pose sequences in the recordings the pose estimates were observed to be okay. It is to be tested however, if these better alignments are of quality high enough to serve as input for any chosen depth up-sampling. State-of-the art visual odometry applies both streams to provide a better pose estimation \citep{Whelan13icra}. The possibility of applying machine learning methods and segmentation for more accurate calibration and pose estimation is also to be explored \citep{malisiewicz-iccv11}.

%%%% KINFU pose estimation %%%%
\begin{figure}[h!]\centering
	\begin{minipage}[b]{0.49\linewidth}
		\includegraphics[width=\textwidth]{/home/bontius/workspace_local/long640_20130829_1525_200_400/results/really_correct_intr/d138_blended.png}
		\caption{Aligned Kinect data. The estimated intrinsic and extrinsic relations of the camera are well suited for the purpose.}
	\end{minipage}
	\begin{minipage}[b]{0.49\linewidth}
		\includegraphics[width=\textwidth]{/home/bontius/workspace_local/long640_20130829_1525_200_400/results/really_correct_intr/kinfu_depth_138_blended.png}
		\caption{Misaligned KinFu pose estimation. The same parameters are used for the virtual viewpoint alignment.}
	\end{minipage}
	\label{fig:limit_kinfu_pose}
\end{figure}

This project has problems with the limitations introduced by the quality of the RGB camera as shown in \figref{fig:kinect_rgb_zoom}. This is only a practical limitation though, since the theoretical enables the application of high quality SLR cameras for refinement.

\begin{figure}[h!]\centering
        \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/kinect_rgb_zoom.png}
        \caption{Kinect RGB image quality at 1280 x 1024 resolution}
        \label{fig:kinect_rgb_zoom}
\end{figure}

\par The strengths and weaknesses of the chosen and applied depth super-resolution technique introduced by \citep{cvpr-07-qingxiong-yang} have been well demonstrated in \secref{sec:results}. Its largely dependent on the alignment of its input images and is sensitive to non-geometric texturing. The latter has most noticeable impact observable around the sticker on the computer in \figref{fig:keyboard2_192_final}, or by the checker-board pattern in  \figref{fig:yang_checkerboard}. The method also requires fixed and hand-tuned parameters that allows limited adaptation to the data. The used parameters of the cross-bilateral filtering method were selected by creating a real-time visual feedback tool with the appropriate trackbars. This is not suitable for frame-by-frame usage, especially since the filtering is applied to the cost volumes and not the visual input frames. A limitation conveying large impact on our solution is the fact, that the filtering approach does not introduce new depth levels, as verified by experiment.
It is claimed by \citep{guided_filter}, that cross-bilateral filtering introduces an unwanted edge inversion effect, this effect is slightly observable in our output images. The direction of a more state-of-the art filtering technique using hybrid Euclidean-geodesic filters introduced by \citep{Gastal:2012} is to be explored.
    
\section{Future work}
\label{sec:future_work}

\par The experiences collected along the {\it Chapters \ref{chp:validation} and \ref{chp:discussion}} allow us to identify the most critical points of the algorithm needing improvement to continue with. The simulated and missing parts of the planned pipeline has to be solved. 

\par The calculation of the actual position of the virtual viewpoint of a new acquisition was simulated during this project. Bundle adjustment implemented in \citep{SnavelySS06} or \citep{vsfm} can be used to align the new RGB data with the available snapshots from the Kinect stream. The new sparse and stored dense reconstruction model can than be aligned by an ICP type algorithm, so that the estimated new camera pose can be transformed between the two coordinate systems.

\par In terms of innovation, the possibilities of determining a decision logic that controls whether the individual parts of the here shown, up-sampled results should be integrated into the mesh. The task of selection of the best viewpoint for a given 3D point and a set of existing camera poses points beyond greedy approaches of minimising distance and maximising normal angle. Ideas along the lines of \citep{Buehler:2001} and \citep{keller13realtime} have to be explored. Preceding work i.e. in \citep{Hoiem:2011} has been performed to handle the problems around occluded regions.
\par Another, possibly even more effective approach is to investigate, what conditions have to be fulfilled to be able to use the up-sampling algorithm from an ideal, normal directed viewpoint by taking the information conveyed in the new colour image into account through ray-casting it to the smooth mesh surface.

\par As identified earlier, intelligent alignment of the new recordings and the existing surface representation has to be performed in order to implement the simulated part of our pipeline. Ideas presented by \citep{Herrera:LearnedJointMRF} are to be investigated. The area of merging the up-sampled and selected depth details with the original model has to be solved using the knowledge of the geometry processing community.

\par Although the filtering technique applied provided promising results in terms of granularity of details. However the difficulties arising due to the characteristics of the approach and proposed solutions for earlier in this section might indicate, that better filtering approaches, or other up-sampling methods have to replace the currently used technique. \\

\par With the development of the domain of multi-view stereo the quality of surface reconstructions achieved might reach the point, where the basis of the solution can be exchanged for an even more commonly accessible recording device in form of a smartphone, or possibly a smartphone with a 3D camera. By moving the computation to the cloud, a  crowd sourcing based follow-up project can target the reconstruction of large scale, public areas. A solution highly suitable for the purposes of cultural heritage preservation.




%\section{System plan}
%	\begin{itemize}
%		\item Calibration
%			\begin{itemize}
%				\item Depth image based
%				\item IR image based
%				\item Undistort
%			\end{itemize}
%		\item Low resolution 3D reconstruction
%			\begin{itemize}
%				\item Multi View Stereo (VSFM, Bundler, Photosynth, etc.)
%				\item Kinect Fusion, PCL::Kinfu
%			\end{itemize}
%		\item Pose estimation of new input
%			\begin{itemize}
%				\item VSFM ( for RGB input )
%				\item ICP ( for RGB-D input )
%				\item Gyro+IMU (a noisy graph of acquired data)
%			\end{itemize}
%		\item Mesh subdivision (Linear, Butterfly, Catmull-Clark)
%		\item 2D projection
%			\begin{itemize}
% 				\item Raycasting using Octree
%				\item GLSL projection
%			\end{itemize}
%		\item Yang filtering
%			\begin{itemize}
%				\item Iterative Cross Bilateral filtering
%				\item Subpixel accurracy
%			\end{itemize}
%		\item Mesh enhancement by backprojection
%	\end{itemize}
%	
%\section{System design}
%	\begin{itemize}
%		\item Implementation details and takeaway experience of "System plan" elements
%		\item Merge into previous?
%	\end{itemize}
%	
%\section{Evaluation}
%	\begin{itemize}
% 	\item Experiments
%		\begin{itemize}
%			\item Calibration
%				\begin{itemize}
%					\item Kinect built in calibration
%					\item Bogouet calibration with and without lens distortion (PARAMETERS explained)
%					\item Undistort effectivity (project undistorted depth map to 3D)
%				\end{itemize}
%				
%			\item Filtering
%				\begin{itemize}
%					\item CrossBilateral filter vs. Bilateral
%					\item Yang vs. CrossBilateral (PARAMETERS explained)
%					\item Trilateral, Guided, Pixel Weighted Average Strategy (Garcia et al,ImProc, 2010) - {\bf IF there's time}
%				\end{itemize}
%			\item Kinect fusion related
%			\begin{itemize}
%				\item Kinfu vs. Kinfu w/ filtering turned OFF
%				\item Kinfu voxel grid resolution ($386^3$,$512^3$,$640^3$)
%				\item Kinfu + Yang( original kinect depth frames )
%				\item Kinfu + Yang( "arbitrary pose" )
%			\end{itemize}
%		\end{itemize}
%	\item Existing libraries used
%	\item Data capturing details
%	\item No existing datasets compared (no time)
%
%	\end{itemize}
%
%\section{Results}
%\begin{itemize}
%	\item Upsampling with original depth frames
%	\item Upsampling with simulated "arbitrary pose"
%
%	\item original yang images
%	\item subpixel refinement
%	
%	\item So, Yang is is not suitable for this resolution, it only could improve, when the input was much noisier
%	\item Different approaches can be plugged in to this framework to enhance depth
%\end{itemize}
%\section{Conclusions, Future work}
%	\begin{itemize}
%		\item 3D reconstruction improvements
%			\begin{itemize}
%				\item Improve on smoothing effect of voxel grid weights
%				\item Pose estimation improvements
%			\end{itemize}
%		\item RGB high-resolution capture, vsfm pose estimation 
%		\item Enhancement by best pose selection from input image collection for given pixel/region
%	\end{itemize}
 
%\bibliographystyle{plainnat}
\bibliography{litrev}
	
\end{document}


% \lstset{style=customcpp}
%\begin{lstlisting}
%__device__ float yangRangeDist( float4 a, float4 b, float sigma )
%{
%    float mod = ( fabs(b.x - a.x) +
%                  fabs(b.y - a.y) +
%                  fabs(b.z - a.z)  ) / 3.f;
%    return __expf(-mod / sigma);
%}
%template <typename T>
%__global__ void
%d_cross_bilateral_filterF( T *dOut, int w, int h, size_t outPitch,
%                           float range_sigma, int r, bool onlyZeros = false )
%{
%    int x = blockIdx.x*blockDim.x + threadIdx.x;
%    int y = blockIdx.y*blockDim.y + threadIdx.y;
%
%    if (x >= w || y >= h) return;
%
%    float sum = 0.f, factor = 0.f, t = 0.f;
%    float4 guideCenter = tex2D( guideTex, x, y );
%    T      centerPix   = fetchTexture<T>( x, y );
%
%    // check for early exit
%    if ( onlyZeros && (centerPix != 0.f) )
%    {
%        dOut[y * outPitch + x] = centerPix;
%        return;
%    }
%
%    // estimate cost volume
%    for ( int i = -r; i <= r; ++i )
%    {
%        for ( int j = -r; j <= r; ++j )
%        {
%            // read depth
%            T curPix = fetchTexture<T>( x+j, y+i );
%            // skip, if no data
%            if ( onlyZeros && curPix == 0.f )
%                continue;
%            // read rgb
%            float4 guidePix = tex2D( guideTex, x + j, y + i );
%            // estimate weights
%            factor = cGaussian[i + r] * cGaussian[j + r] *
%                     yangRangeDist( guidePix, guideCenter, range_sigma );
%            // accumulate
%            t   += factor * curPix;
%            sum += factor;
%        }
%    }
%    if ( sum > 0.f )
%        dOut[y * outPitch + x] = t / sum;
%    else
%        dOut[y * outPitch + x] = centerPix;
%}
%
%\end{lstlisting}


