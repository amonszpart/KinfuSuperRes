\documentclass{ucl_thesis}

%twoside for double page printing
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{color}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage{algorithmic}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[square]{natbib}

\usepackage[font=footnotesize,labelfont=bf,singlelinecheck=on]{caption}
\usepackage[hyphens]{url}

\usepackage{amsmath}
% Uncomment this to activate the TikZ library, which is useful for fancy block-diagrams.
%    \usepackage{tikz}
%    \usetikzlibrary{positioning,arrows,shapes.misc}
\usepackage{multirow}

%\numberwithin{algorithm}{chapter}
%\usepackage{epsf}
\usepackage{fancyvrb}

%\documentclass[12pt,a4paper]{article}
%\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
%\usepackage{amsmath}
%\usepackage{amsfonts}
%\usepackage{amssymb}
%\usepackage{graphicx}
%usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
%
%\usepackage[]{natbib}

% // TODO: add pagenumbers

%% LISTINGS
\usepackage{color}
\usepackage{listings}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\definecolor{purple}{rgb}{0.28,0.23,0.5}
\lstdefinestyle{customcpp}{
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  frame=L,
  xleftmargin=\parindent,
  language=C++,
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\bfseries\color{green},
  commentstyle=\itshape\color{purple},
  identifierstyle=\color{blue},
  stringstyle=\color{orange},
  numbers        = left,
  stepnumber     = 5,
}
%%

%% figure template
\newcommand{\myfig}[6]{%
\begin{figure}[h!]\centering%
	\begin{minipage}[b]{0.49\linewidth}\centering%
		\includegraphics[width=\textwidth]{#2}%
		\caption{#3}%
		\label{fig:#1}%
	\end{minipage}%
	\begin{minipage}[b]{0.49\linewidth}\centering%
		\includegraphics[width=\textwidth]{#5}%
		\caption{#6}%
		\label{fig:#4}%
	\end{minipage}%
\end{figure}%
}
%%

%% shortcuts
%Malcolm added some custom commands here - you can make your own!
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\figref}[1]{(Fig. \ref{#1})}
\newcommand{\secref}[1]{(Section \ref{#1})}
\def\etal{{et~al.}}
\def\ie{{\it i.e.,\ }}
\def\etc{{\it etc.,\ }}
\def\eg{{\it e.g.,\ }}
\def\vs{{\it vs.\ }}
\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
%%

\author{Aron Monszpart}
\title{Hand-held Scanning of Surface Details}
\def \supervisor {Dr. Gabriel Brostow}
\date{September 2013}

\begin{document}

\bibliographystyle{plainnat}
\maketitle
\numberwithin{algorithm}{chapter}
\setcounter{page}{1}
\pagenumbering{roman}
\pagestyle{plain}

% Notice how the star "*" is used throughout LaTex to modify the normal behavior. For example, \section* instead of \section 
% tells LaTex NOT to number this section.


\newpage
\section*{Abstract}
I've done some stuff
%\input{abstract.tex}


\newpage
\section*{Acknowledgements}
Dr. Gabriel Brostow, Dr Neill Campbell, Dr. Oisin Mac Aodha, Fabrizio Pece, Malcolm Reynolds, Clement Godard, Peter Rennert and the UCL PRISM group

%\setcounter{tocdepth}{2}
\tableofcontents
\listoffigures
%\listoftables
%\listofalgorithms
\newpage

\setcounter{page}{1}
\pagenumbering{arabic}
\pagestyle{plain}

\chapter{Introduction and Background} 

\par The targeted system attempts to solve the problem of 3D reconstruction of fine surface details in static scenes. We have seen various examples of sparse 3D reconstruction methods using the achievements of multi-view stereo, structure from motion and SLAM techniques in \secref{sec:SFM}. In recent years, a large leap has been made in dense 3D reconstruction by methods in \secref{sec:kinfu}. By designing this system we attempted to address inaccurracies produced by the above mentioned methods to refine high frequency details in small segments of the reconstructed models.

\label{chp:background}
	\begin{itemize}	
		\item Motivation
		\item Overview of approach
		\begin{itemize}
			\item Calibration
			\item Low resolution 3D reconstruction
			\item Pose estimation of new input
			\item Mesh subdivision and 2D projection
			\item Depth upsampling
			\item Mesh enhancement by backprojection
		\end{itemize}
	\end{itemize}

\chapter{Related work} 
\label{chp:related_work}

\section{Multi view stereo}
\label{sec:mvs}
Spatioangular lightfields, disney: \citep{Kim:2013}

\section{Calibration}
\label{sec:lit_calib}

One of the initial steps of the setup of every multi-camera system has to be calibration. The expression calibration can be interpreted with several meanings. In this document we refer to calibration as the preprocessing step that captures the characteristics of the physical recording system accounting for, amongst others, inaccuracies in the physical manufacturing processes of the cameras. A successful calibration enables a correspondence to be calculated between data captured at the same time with the different cameras. This correspondence is data dependent in the case of depth cameras, and thus calculated every frame.\\

A wider interpretation of calibration includes the calculation of a nearly global position of the recording system. It usually is incorporated by the relative position estimation between the observed scene and the recording system, which can be viewed as global in case the global position of the observed scene is known. \\

Most previous works targeting to develop a highly accurate calibration system agree on the computational parameter model called pinhole-camera model \figref{eq:pinhole}. The model specifies the parameters: focal length ($f_x$, $f_y$), principal point ($c_x$, $c_y$) and skew ($\gamma$).

\begin{figure}[h!]
\begin{equation}
\lambda
\underbrace{
	\left[\begin{array}{ccc}
	    x \\
	    y \\
	    1 \\
	\end{array} \right]
}_{
	{\bf \tilde{x}}_{image}
}
%
=
%
\underbrace{
	\left[\begin{array}{cccc}
			f_x & \gamma & c_x & 0 \\
			0   & f_y    & c_y & 0 \\
	 		0   & 0      & 1   & 0 \\
	\end{array} \right]
}_{
	\left[\begin{array}{cc}
		\bf{\Lambda} & \bf{0}
	\end{array} \right]
}
%
\underbrace{
	\left[\begin{array}{cccc}
	    r_{1,1} & r_{1,2} & r_{1,3} & t_1 \\
	    r_{2,1} & r_{2,2} & r_{2,3} & t_2 \\
	    r_{3,1} & r_{3,2} & r_{3,3} & t_3 \\
	    0       & 0       & 0       & 1 \\
	\end{array} \right]
}_{
	\left[\begin{array}{cc}
		\bf{\Omega}     & \bf{\tau} \\
		\bf{0}^{\it{T}} 	& 1		      \\
	\end{array} \right]
}
%
\underbrace{
	\left[\begin{array}{c}
	    u \\
	    v \\
	    w \\
	    1 \\
	\end{array} \right]
}_{
	{\bf \tilde{x}}_{world}
}
\end{equation}
\caption{Correspondence between a point in global world coordinates (X,Y,Z) and camera image coordinates (u,v) according to the pinhole camera model. Source: UCL 2012 Machine Vision lecture notes. }
\label{eq:pinhole}
\end{figure}

Initial works \citep{Melen:1994}, \citep{Weng:1992} use the combination of linear, and non-linear minimisation techniques to calculate physically based camera parameters. In [9] non-physical implicit parameters are used in a two-step optimisation approach. The baseline calibration technique used in this project relies on the four-step calibration approach presented by \citep{Heikkila:1997}. They introduced a technique to estimate a four component set of distortion parameters in addition to the intrinsics of the pinhole camera. These parameters account for the radial ($k_1$, $k_2$) and tangential ($p_1$, $p_2$) distortion of the camera model. In parallel \citep{Zhang00} published a different camera calibration method only relying on two radial distortion parameters. A widely used implementation of the theoretical achievements of the above works is incorporated in Jean-Yves Bouguet's "Camera Calibration Toolbox for Matlab" \citep{calibration_bouguet}. Here a third order radial distortion parameter $k_3$ is introduced for higher accurracy. The computational model optimised for is described in \figref{eq:distortion}.

\begin{figure}[h!]
    \begin{equation*}
	\left[\begin{array}{c}
		u_{camera} \\
		v_{camera} \\
		w_{camera} \\
	\end{array} \right]
	=
    	\left[\begin{array}{c|c}
	    	{\bf \Omega} & {\bf \tau}
	\end{array} \right]
	\left[\begin{array}{c}
	    u \\
	    	v \\
	    w \\
	\end{array} \right]
    \end{equation*}
    
    \begin{equation*}
	{\bf x}_{normalized} = 
 	\left[\begin{array}{c}
	    \frac{ u_{camera} }{ w_{camera} } \\
	    \frac{ v_{camera} }{ w_{camera} } \\
	\end{array} \right]; 
	~r = length(	{\bf x}_{normalized} )
    \end{equation*}

    \begin{equation*}
	{\bf x}_{undistorted}
	=
	(1 + k_1 r^2 + k_2 r^4 + k_3 r^6 ) {\bf x}_{normalized} 
	~+~ 
	\left[\begin{array}{c}
	    2 p_1 {\bf x}_{normalized,x} {\bf x}_{normalized,y} + p_2 ( r^2 + 2{\bf x}_{normalized,x}^2) \\
	    p_1(r^2+2{\bf x}_{normalized,y}^2) + 2 p_2 {\bf x}_{normalized,x} {\bf x}_{normalized,y}\\
	\end{array} \right]
	\end{equation*}
    \begin{equation*}
		{\bf \tilde{x}}_{image} = {\bf\Lambda} ~ {\bf \tilde{x}}_{undistorted}
    \end{equation*}
    
    \caption{Distortion parameters in the pinhole camera model}
    \label{eq:distortion}
\end{figure}

The achievements of the toolbox were further developed in Intel's \citep{calibration_opencv} package . Additionally, the Graphics and Media Lab of the Lomonosov Moscow State University published a C++ and Matlab calibration toolbox known as \citep{calibration_gml}.
\citep{calibration_rgbdemo}

herrera 13


%http://burrus.name/index.php/Research/KinectCalibration
%vrui
%ros http://wiki.ros.org/kinect\_node/Calibration
%ellipses



\section{Super-resolution}
\label{sec:super_resolution}

% not depth:
Joint bilateral upsampling PCL: \citep{Kopf:2007}

The bilateral filter has been used previously for various image pro-
cessing tasks. Durand and Dorsey [2002] applied the bilateral fil-
ter to HDR tone mapping and also described a fast approximation,
which was recently improved upon [Paris and Durand 2006; Weiss
2006].
Ramanath and Snyder [2003] used the bilateral filter in the context
of demosaicking to improve edge sensitivity. Their method is re-
stricted to Bayer patterns with a fixed small upsampling factor, and
does not use a guidance image as we do.
%very close:
yang

\citep{MatsuoFI13} \\
\citep{} TUVienna paper of selfsimilarities

In \citep{Silberman:ECCV12} ... they give a very good kinect dataset for segmentation, use RANSAC and implement cross bilateral filtering



\section{Kinect fusion papers}
\label{sec:kinfu}

%vladlen:
\citep{Zhou:2013}
% mr siggraph 13 kinfu:
\citep{Chen:2013:Scalable_volumetric}
% kintinous 2
\citep{Whelan13iros}
% Kintinuous1
\citep{Whelan12rssw}
% lefloch
Point based Fusion:
\citep{keller13realtime}

Pose estimation improvement: \citep{Whelan13icra}
Multi scale depth: \citep{Fuhrmann:2011}  keywords = {depth map integration, hierarchical signed distance field, marching tetrahedra, multi-scale depth map fusion, multi-view stereo depth maps, surface reconstruction},
}

\section{<planned>}

%	\begin{enumerate}
%		\item 3D reconstruction using Multi-view stereo
%		\begin{itemize}
%			\item Curless and Levoy
%			\item Rusinkiewicz
%			\item Szeliski
%			\item Agarwal
%			\item Snavely
%			\item Seitz
%			\item Furukawa and Ponce
%			\item Strecha
%			\item etc. from literature review (got good marks for it)
%			\item Kim et al: Scene reconstruction from High Spatio-Angular Resolution Light Fields (Siggraph '13)
%		\end{itemize}
%
%		\item 3D reconstruction using RGB-D
%		\begin{itemize}
%			\item Voxelgrid based
%			\begin{itemize}
%				\item Newcombe: Kinect fusion
%				\item Whelan: Kintinuous and other loop closure papers
%				\item Vladlen: Dense scene reconstruction with points of interest
%				\item etc.
%			\end{itemize}
%
%			\item Height-field based, layered
%			\begin{itemize}
%				\item Adaptive voxelgrid based, scene scale aware (Freiburg)
%				\item etc.
%			\end{itemize}
%
%			\item Point based
%			\begin{itemize}
%				\item Damien Lefloch
%				\item etc.
%			\end{itemize}
%
%			
%		\end{itemize}
%			
% 		\item Pose estimation
%		\begin{itemize}
%			\item some ICP references
%			\item Gyro papers
%			\item MVS already earlier
%		\end{itemize}
%		
%		\item Upsampling
%		\begin{itemize}
%			\item RGB superresolution
%			\item Depth superresolution
%			\begin{itemize}
%				\item Non-guided
%				\begin{itemize}
%					\item Learning based
%						\begin{itemize}
%							\item Mac Aodha et al. '12
%							\item etc.
%						\end{itemize}
%					\item Other
%					\begin{itemize}
%						\item Hornacek et al.: Depth Super Resolution by Rigid Body Self-Similarity in 3D, CVPR '13
%						\item etc.
%					\end{itemize}
%				\end{itemize}
%				
%				\item Guided
%				\begin{itemize}
%					\item Joint Bilateral
%					\begin{itemize}
%						\item Yang '07
%						\item etc.
%					\end{itemize}
%					\item MRF based
%					\begin{itemize}
%						\item Diebel and Thrun
%						\item Schuon et al.
%						\item Park et al. '11
%						\item Choi et al. '12
%						\item Shen and Cheung CVPR '13 (Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras)
%						\item etc.
%					\end{itemize}
%					
%					\item Spatio-temporal filtering
%					
%				\end{itemize}
%			\end{itemize}
%		\end{itemize}
%		
%		\item Commercial 3D scanners
%		\begin{itemize}
%			\item MVS based: Photosynth, Adobe, etc.
%			\item Laser scanner systems (industrial)
%			\item Kinect based: Scanec, Reva, ReconstructMe, ...
%		\end{itemize}
%		
%		\item Seen/used calibration references
%		\item Subdivision references
%		\item Efficient raycasting references
%		\item (+ Categorize all the ref e-mails from Gabriel, Oisin, Reading groups, etc.)
%	
%	\end{enumerate}

\chapter{MySystem} 
\label{chp:my_system}

\par The targeted system attempts to solve the problem of 3D reconstruction of fine surface details in static scenes. We have seen various examples of sparse 3D reconstruction methods using the achievements of multi-view stereo, structure from motion and SLAM techniques in \secref{sec:SFM}. In recent years, a large leap has been made in dense 3D reconstruction by methods in \secref{sec:kinfu}. By designing this system we attempted to address inaccuracies produced by the above mentioned methods to refine high frequency details in small segments of the reconstructed models. \\

\par The system designed acknowledges the power of SLAM performed on the dense model incorporated in the areas of RGB-D based methods. We also recognize the power of other research areas, where inter-sensory information is used to enhance output quality. We hypothesize that there is information collected by the capturing system but is discarded too early in the reconstruction pipeline of a standard RGB-D system. We looked for methods that enable this information to be reintegrated into the results of the reconstruction at later processing steps. The solution was designed based on the alleged potential of iterative, cost-volume based joint bilateral filtering with sub-pixel accuracy in \citep{cvpr-07-qingxiong-yang}. \\


\begin{figure}[h!]\centering
    \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/sys_overview.png}
    \caption{System overview}
    \label{fig:sys_overview}
\end{figure}
//TODO: What problem does my system solve?
!any RGB pose refinement!
//TODO: Pipeline image

\section{Assumptions}
\label{sec:assumptions}

\par When designing a system solving a specific problem, it is important to firmly decide what assumptions are made about the input, what problems are targeted to be solved and what problems are out of the scope of the planned method. We target to design a system, that reconstructs fine surface details of static scenes with mostly Lambertian surfaces allowing offline post-processing. \\

\par The fine surface details of a static scene are to be reconstructed. The problems arising when dealing with dynamic scenes are out of scope for this solution. Rigidity is a common assumption in the field of 3D reconstruction both for multi-view stereo and structured lighting methods. An essential building block of our concept is the possibility to later acquire new data about missing details and integrate it into the reconstruction. Since pose estimation of latter acquisitions is a substantial problem on its own, the assumption about static scenes seems reasonable to make. \\

\par Surfaces with close-to Lambertian reflectivity are considered. During experiments refractive and reflective surfaces appeared in the scene, but the main focus of the system development addresses problems emerging around fine details on mostly Lambertian surfaces. \\

\par The solution will perform post-processing offline. The planned key contribution of the work is to acquire surface details at a finer level than current state-of-the art solutions are capable of. As generally in all fields of computer science, a trade-off between quality and speed has to be made. Thus, the planned solutions operate using offline processing and refinement steps. Precedence in similar projects of using offline processing as capabilities of cloud computing has been observed in applications as Adobe Catch \citep{AdobeCatch} or Microsoft Photosynth \citep{Photosynth}. State-of-the art large-scale RGB-D reconstruction algorithms also may operate off-line, as \citep{Zhou:2013}. \\

\section{Calibration}
\label{sec:calibration}

What is calibration, pinhole camera model, different notations (oulu, etc.), 
with IR, without IR
USB bus problems, overlayed image

% Input rgb and depth
\begin{figure}[h!]\centering
    \begin{minipage}[b]{0.49\linewidth}
        \includegraphics[width=\textwidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/SuperRes-NI-1-5/build/out/imgs_20130725_1809/img8_00000001.png}
        \caption{Input RGB}
    \end{minipage}
    \begin{minipage}[b]{0.49\linewidth}
        \includegraphics[width=\textwidth]{/media/Storage/Dropbox/UCL/project/results/presentation_300713/dep8_00000001.png}
		\caption{Input depth}
    \end{minipage}
\end{figure}

% Simple alignment
\begin{figure}[h!]\centering
	\begin{minipage}[b]{0.49\linewidth}
		\includegraphics[width=\textwidth]{/media/Storage/Dropbox/UCL/project/results/presentation_300713/dep16AndRgb_00000001.png}
		\caption{No alignment}
	\end{minipage}
	\begin{minipage}[b]{0.49\linewidth}
		\includegraphics[width=\textwidth]{/media/Storage/Dropbox/UCL/project/results/presentation_300713/dep16AndRgb_00000000.png}
		\caption{"Built-in alignment"}
	\end{minipage}
\end{figure}


\begin{figure}[h!]\centering
    \begin{minipage}[b]{0.49\linewidth} \label{fig:ir}
        \includegraphics[width=\textwidth]{/media/Storage/Dropbox/UCL/project/results/presentation_300713/irAndDep8_00000001.png}
        \caption{Default IR and depth overlay}
    \end{minipage}
    \begin{minipage}[b]{0.49\linewidth}
        \includegraphics[width=\textwidth]{/media/Storage/Dropbox/UCL/project/results/presentation_300713/offsIrAndDep8_00000001.png}
        \caption{IR offset by radius of convolution kernel}
    \end{minipage}
\end{figure}

\begin{figure}[h!]\centering
        \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/SuperRes-NI-1-5/build/out/imgs_20130725_1809/img8_00000001_mapped.png}
        \caption{Mapping from 640x480 depth to 1280x1024 RGB after calibration}
\end{figure}


default image, built in alignment image, good image\\
calibration papers, and their comparison of optimisation techniques\\
calibration with different sources (depth and acryl, ir, ellipses)\\
cuda mapping\\


\subsection{takeaway}
Ir image with and without projector\\
large checkerboard vs. small checkerboard\\
all positions (mosaic image?)\\
checkerboard sze 
All the squares must be clearly visible (unoccluded).
Use a tripod
Take 25 images and more
Use a paper size ".3" and more
Square size is 3-5 cm
The chessboard on the images must be located in the all places of the camera matrix

\section{3D reconstruction}

The task of accurate real-time mapping of complex and
arbitrary indoor scenes in variable lighting conditions was claimed to have been solved by \citep{Newcombe11}. They list using only a
moving low-cost depth camera and commodity graphics hardware to their contributions as well. There have been made many improvements to the method since it's initial publication detailed in \secref{sec:kinfu}. We base the hypothesis of our work on the claim, that RGB-D reconstruction's attention to the complexity of the scenes can be developed further. Therefore, during the design of the system this algorithm was treated as baseline, its results were reproduced, carefully inspected and an attempt was made to improve upon them. During the extensive research performed to assess the capabilities of derivatives of this algorithm to reconstruct the targeted fine details we did rarely come across projects giving reason to attempt to reproduce their results instead, and treat them as baseline. However, in the future point-based fusion concepts introduced by \citep{keller13realtime} is worth taking into account to improve the starting point of our algorithm.

\begin{figure}[h!]\centering
	\begin{minipage}[b]{0.33\linewidth}
		\includegraphics[width=\textwidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/kinectfusion_example.png}
		\caption{Image from \citep{KinectFuSDKExample}}
		\label{fig:kinectsdk_example}
	\end{minipage}
	\begin{minipage}[b]{0.33\linewidth}
		\includegraphics[width=\textwidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/kinfu_baseline00.png}
		\caption{Mesh reconstruction of scene recorded by us. Reconstruction using PCL's Kinfu with our modifications}
		\label{fig:kinfu_baseline}
	\end{minipage}
	\begin{minipage}[b]{0.33\linewidth}
		\includegraphics[width=\textwidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/lefloch_largescale_cropped.png}
		\caption{\citep{keller13realtime}}
		\label{fig:lefloch}
	\end{minipage}
\end{figure}



\subsection{System input}
\par The system design applied in RGB-D SLAM 3D reconstruction methods aligns well to our initial concept of deign of solution. This is true both in terms of processing pipeline and hardware and software tools applied. The scene acquisition happens using sensors providing both chromatic and distance information. It is a common assumption to use a low-cost, usually hand-held device with integrated sensors as the Microsoft Kinect or ASUS Xtion Pro. A calibrated system of a time-of-flight camera and an RGB camera is another usual system setup. We used a calibrated Microsoft Kinect device. \\

\par An RGB-D SLAM 3D reconstruction system operates with at least two input streams. A data stream containing chromatic information about the observed scene, usually encoded in 8-bits per channel, and another information source conveying information about the spatial relations of the scene encoded in some form of distance information.
\par The Microsoft Kinect is capable of conveying three channels of chromatic data encoded in a 24-bit data-stream. The most important available resolution settings with the used OpenNI Middleware are 640 x 480 pixels at \mytilde 30 frames per second and 1280 x 1024 pixels at \mytilde 15 frames per second. The quality of the RGB image seems to suffer from the problems Bayer filtering type methods introduce, as it can be seen in \figref{fig:kinect_rgb_zoom}. This introduces slight corner detection problems at calibration time, and more significant problems when applying the up-sampling algorithm as discussed in the limitations section \secref{sec:limitations}. 
\par The depth stream is available in the resolution 640 x 480 at \mytilde 30 frames per second. The critical analysis of the quality of the depth map has been researched in many projects, amongst others by modelling the noise of the sensor \citep{NguyenIL12}, or applying temporal filtering \citep{RGBZcamera}. It is considered out of the scope of the current project to pre-process the depth stream before applying it in the system pipeline. \\

\par To establish working volume boundaries and area of focus, the operating range and optimal scanning distance of the used depth acquisition device has to be known. According to \citep{Kinect_ms} the Kinect depth sensor has a minimum range of 800 mm and a maximum of 4 m. The Kinect for Windows Hardware can work in Near Mode providing a range of 500 mm to 3 m. By sampling the depth maps acquired during our data collection process it can be stated, that these characteristics can be achieved using the OpenNI Middleware as well, depths around 500 mm have been observed. 
\par In future work, the \citep{Kinect_nyko_zoom} might provide possibility to zoom in further on the scene and acquire more accurate depth data about fine details, meanwhile the distortion effect of the additional lens has to be evaluated as well.

\subsection{Processing algorithm}
\par An open-source implementation of the classical Kinect Fusion algorithm was employed to retrieve a smooth 3D reconstruction from the data acquired. The \citep{PCL} has implementation both for the original small to mid-scale algorithm named "Kinfu" and an implementation capable of handling scenes with larger extent called "Kinfu Large Scale". Due to the targeted proof-of-concept nature of our project the computationally slightly less intensive Kinfu implementation was used as starting point. The sub-project was recompiled to a separate, modifiable library allowing slight customisations to the algorithm (project "MyKinfuTracker" in the source code attachment). Algorithmic changes or implementation of later publications detailed in \secref{sec:kinfu} incorporating improvements to the concept are considered out of scope for this project.\\

\par The outline of PCL's Kinfu algorithm is as follows: All depth data streamed from a Kinect sensor is merged into a global implicit surface model stored in a fixed resolution voxel grid in the GPU's memory. In each voxel grid cell the distance to the closest surface is stored as a truncated signed distance function (TSDF). Subject to convention the two signs represent the inside respectively outside of the assumed non-degenerate mesh. 
\par Upon arrival of new depth data a 3D point vertex and normal map is generated using camera the intrinsics. Correspondence to the stored mesh represented by the stored TSDF is calculated using ray-casting and the point cloud represented by the vertex map is used to estimate the new camera pose with respect to the current model. This is performed using a coarse-to-fine iterative closest point algorithm. 
\par The point cloud is then regenerated using the new camera pose and the appropriate cells of the voxel grid are updated. A new vertex and normal map is generated to prepare for the next arriving frame. The update method has large impact on our project goals. In \secref{sec:3d_sys_output} the role is discussed further with other parameters of the algorithm. \\

\par By recompiling PCL's Kinfu solution to a stand-alone library we prepared the implementation for future modifications. This allowed us to experiment with the different parameters detailed in the next section. These were hard coded into the solution in order to enable CUDA code to perform optimally. The most important modification besides change of parameters was to turn off the initial bilateral filtering of the input depth to enable higher attention to detail. The effectivity of this modification was suppressed by the averaging nature of the TSDF update method. Since the rest of the implementation assumed filtered depth maps with no missing information, a bilateral filtering step was reintroduced, that fills only pixels with missing data. A more accurate solution would require missing data to be disregarded at later stages of the pipeline.

\subsection{Configuration and system output}
\label{sec:3d_sys_output}

The results of the reconstruction stored in the voxel grid as a TSDF volume can be exported to several output formats. A GPU implementation of the classical marching cubes algorithm \citep{Lorensen:1987} exports the mesh into a point cloud. A triangle mesh can also be exported using the same method and connecting neighbouring grid cells to faces. The storage format is greedy storing all three vertices for every face. This introduced algorithmic challenges when performing subdivision on the mesh at later stages of the pipeline. The results can also be visualized using ray-tracing and Phong shading. We used the triangle mesh as output of this system in the pipeline.

\par Configuration parameters can be used to tweak the system to yield more optimal results, a reconstruction with higher attention to detail. The most important parameters of the Kinect Fusion algorithm are voxel grid resolution, working volume size, truncation distance, maximum movement threshold and weight of new information.

\par The resolution of the voxel grid and working volume size has the highest impact on the accuracy of fine details in the reconstruction. The parameters used during reconstruction resulted in a spatial resolution of $3 m / 512$ or $3 m / 640$ meaning an average accuracy of 5.85 mm respectively 4.68 mm. This implied a $3m^3$ working volume represented by $512^3$ respectively $640^3$ voxels. Higher values were bounded by hardware constraints (GPU memory size). The quality of the final output of our system was compared relatively to the output of the 3D reconstruction in the \secref{sec:discussion_results}.

\par The truncation distance has an impact on the accuracy of cavities and narrow details. The importance of this parameter is relevant when performing the raycasting steps of the algorithm. Since our solution does not need real-time performance this value could be increased to result in results with higher accuracy. Thus the output quality is not limited by this parameter, but by the spatial resolution of the voxel grid.

\par The maximum movement threshold is relevant to the pose estimation step of the algorithm. This parameter ensures, that the pose estimation algorithm did find a good match between the model and the new frame. By increasing the tolerance for movement between frames, we would allow less accurate pose estimations and faster camera movement. By decreasing the value more information would get discarded.

\par As stated earlier, the TSDF update method is an important parameter in terms of the reconstruction quality. In the given implementation, the information conveyed by new measurements are averaged giving each measurement equal weight. On the \citep{KinectFuSDKExample} page they say, that the weight parameter {\it "...controls the temporal averaging of data into the reconstruction volume – increasing makes the system a higher detailed reconstruction, but one which takes longer to average and does not adapt to change. Decreasing it makes the volume respond faster to change in the depth (e.g. objects moving), but is noisier overall."}. The equal weighting of all measurements has a large impact on the output introducing a strong smoothing effect discarding fine surface details and giving place to the hypothesis of our project.

\section{Depth super-resolution}

The base hypothesis of our project states, that there is information recorded by the sensors that is discarded early in the pipeline. We claim, that by identifying and reintegrating this information at later stages, the 
In \secref{sec:super_resolution} many methods were overviewed.

1 Input
2 Process: PCL steps short description
\subsection{Configuration and system output}


%\lstset{frame=single,language=C++,tabsize=1,basicstyle=\ttfamily\small,}
\lstset{style=customcpp}
\begin{lstlisting}
__device__ float yangRangeDist( float4 a, float4 b, float sigma )
{
    float mod = ( fabs(b.x - a.x) +
                  fabs(b.y - a.y) +
                  fabs(b.z - a.z)  ) / 3.f;
    return __expf(-mod / sigma);
}
template <typename T>
__global__ void
d_cross_bilateral_filterF( T *dOut, int w, int h, size_t outPitch,
                           float range_sigma, int r, bool onlyZeros = false )
{
    int x = blockIdx.x*blockDim.x + threadIdx.x;
    int y = blockIdx.y*blockDim.y + threadIdx.y;

    if (x >= w || y >= h) return;

    float sum = 0.f, factor = 0.f, t = 0.f;
    float4 guideCenter = tex2D( guideTex, x, y );
    T      centerPix   = fetchTexture<T>( x, y );

    // check for early exit
    if ( onlyZeros && (centerPix != 0.f) )
    {
        dOut[y * outPitch + x] = centerPix;
        return;
    }

    // estimate cost volume
    for ( int i = -r; i <= r; ++i )
    {
        for ( int j = -r; j <= r; ++j )
        {
            // read depth
            T curPix = fetchTexture<T>( x+j, y+i );
            // skip, if no data
            if ( onlyZeros && curPix == 0.f )
                continue;
            // read rgb
            float4 guidePix = tex2D( guideTex, x + j, y + i );
            // estimate weights
            factor = cGaussian[i + r] * cGaussian[j + r] *
                     yangRangeDist( guidePix, guideCenter, range_sigma );
            // accumulate
            t   += factor * curPix;
            sum += factor;
        }
    }
    if ( sum > 0.f )
        dOut[y * outPitch + x] = t / sum;
    else
        dOut[y * outPitch + x] = centerPix;
}

\end{lstlisting}

\section{Combined system pipeline}
My combined pipleine ( this is what I did, to get the data from here to there. Draw attention to GPU implementations (get credit for it) )


Input: Depth (640x480 @ 30 FPS), RGB(1280x1024 @ 15 FPS)
Transfor
[move depth to RGB space]
[cross filter to fill holes]
kinfu 
acquire new rgb
estimate pose
render kinfu model from pose
upsample

GLSL mesh enhancement
kinfu update method

\section{Implementation details}
\subsection{Drivers}
\subsection{Third-party libraries}
simultaneous rgb and IR problems
NVIDIA bilateralfiltering sample
PCL
 pcl bilateral upsampling
 \citep{DCBGridStereo}
 
Renderings made using \citep{Meshlab}.

%%% CHAPTER 5 %%%
\chapter{Validation / experiments}
\label{chp:validation}

%%% SECTION %%%
\section{Structure from motion} 
\label{sec:sfm}

% acerlit image
\myfig
{acerlit_1}
{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/acer_lit00706.jpg}
{A frame of video of backside of a monitor}
{acerlit_mesh}
{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/acer_lit01.png}
{Visual SFM 3D reconstruction of monitor} 

% oil bottle image
\begin{figure}[h!]\centering
	\begin{minipage}[b]{0.49\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/ob00003r2.jpg}
		\caption{A frame of video of oil bottle}
		\label{fig:ob_1}
	\end{minipage}
	\begin{minipage}[b]{0.49\linewidth}\centering
		\includegraphics[height=5cm]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/ob_mesh00.png}
		\caption{Visual SFM 3D reconstruction of oil bottle}
		\label{fig:ob_mesh}
	\end{minipage}
\end{figure}

%%% SECTION %%%
\section{Iterative joint bilateral upsampling with sub-pixel accuracy}
\label{sec:yang}

\begin{figure}[h!]\centering 
        \includegraphics[width=\linewidth]{/media/Storage/Dropbox/UCL/project/results/final/yang_checkerboard/yanged_nonflat.png}        
        \caption{Example run of implementation on checkerboard scene, results without and with texture after 50 iterations.}
        \label{fig:yang_checkerboard}
\end{figure}

In \figref{fig:yang_checkerboard}

Yang reproductions

Yang does not introduce new depths

%%% SECTION %%%
\section{Results}
\label{sec:results}

\begin{figure}[h!]\centering
    \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/keyboard_192_final.png}
    \caption{Close up rendering of keyboard upsampled from skew view angle}
    \label{fig:keyboard_192}
\end{figure}

\begin{figure}[h!]\centering
    \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/keyboard2_192_final.png}
    \caption{Close up rendering of different keyboard upsampled from closer to normal view angle}
    \label{fig:keyboard2_192}
\end{figure}

See meshes on CD

\chapter{Discussion} 
\label{chp:discussion}

\section{Results}
\label{sec:discussion_results}

    - come back to 3's experiments\\
    - interpret experiments\\
    - did you notice, that 2 were good, and 3rd was abd, remember what I sadi in 3.2, that kinfu does it's calibration...well ,that's crap here (threshold), it happend to be good for 2 sequences, but not their thirds \\
-     Pros, cons \\
- PFM format \\

\subsection{Dense or sparse reconstruction}

\par An easy to operate, hand-held scanning device is used. The developed system will most likely use a smartphone with built in inertial measurement unit, gyroscope, magnetometer and GPS sensor. Alternatively a camera with mounted and calibrated motion and orientation sensors can be used. The performance of structured lighting devices as Microsoft Kinect \cite{Kinect} or Asus Xtion PRO \cite{XtionPro} will also be explored. Eventually a higher resolution device as Leap-motion's gesture recognition solution \cite{LeapMotion} will be evaluated. To ensure interchangeability of the sensors, and to get these systems to cooperate to the highest level possible, a well designed framework needs to be developed. The extensibility of the KinectFusion SDK will be tested once it get's released as part of the Windows 8 SDK as promised \cite{SDKKinectFusion}. Alternatives, as the implementation of a Kinect Fusion like functionality in the Point Cloud Library ("Kinfu") will also be tested \cite{KinFu}.
\par The commercial accessibility of the used sensor setup has a slightly increased emphasis at this stage of the project. Low cost and out of the box operability would open the possibilities for crowd sourcing in follow-up projects, something that has been targeted by many other research labs as Microsoft (at Techfest 2013) or the University of Washington (Photocity, Pointcraft).

/// The current sensor pose is simultaneously obtained by
tracking the live depth frame relative to the global model using a
coarse-to-fine iterative closest point (ICP) algorithm, which uses
all of the observed depth data available. We demonstrate the advantages
of tracking against the growing full surface model compared
with frame-to-frame tracking, obtaining tracking and mapping results
in constant time within room sized scenes with limited drift
and high accuracy. We also show both qualitative and quantitative
results relating to various aspects of our tracking and mapping system.
Modelling of natural scenes, in real-time with only commodity
sensor and GPU hardware, promises an exciting step forward
in augmented reality (AR), in particular, it allows dense surfaces to
be reconstructed in real-time, with a level of detail and robustness
beyond any solution yet presented using passive computer vision.

\section{Limitations} 
\label{sec:limitations}
	- kinfu memory needs
	- so if Kinfu is not reliable, say it! so if it does not work (pose) than \citep{Whelan13icra} \\
	- misalignment, few frames later, it's great \\
	- calibration \\
    - obvious (Yang on chessboard, it still does) \\
    - summarizing what I discovered \\
    - Kinect RGB quality \\
    \begin{figure}[h!]\centering
        \includegraphics[width=\linewidth]{/home/bontius/workspace/cpp_projects/KinfuSuperRes/thesis/img/kinect_rgb_zoom.png}
        \caption{Kinect RGB image quality at 1280 x 1024 resolution}
        \label{fig:kinect_rgb_zoom}
    \end{figure}
    - TODO: aligned kinect-rgb, but misaligned kinfu frame example
    
    
\section{Future work}
- Using segmentation \citep{Silberman:ECCV12} \\
- Occlusion \citep{Hoiem:2011} \\
- Learning based alignment and depth \citep{Herrera:LearnedJointMRF} \\
- put back into mesh (loookup papers!)
- geom processing papers from PCL surface package

This concept fits well into the idea of using a smartphone as sensor in terms of storage space even when recordings of movement sensors' measurements are made. It also fits the concept of an eventual crowd sourcing follow-up project. 

future work: I advocate Yang for the following reasons, or I discourage, since...or in some situations this, than that \\
Kinect \\
Yang \\
Grid recordings of monitor\\

    - zoom lense: \citep{Kinect_nyko_zoom}
In future work, the \citep{Kinect_nyko_zoom} might provide possibility to zoom in futher on the scene and acquire more accurate depth data about fine details, meanwhile the distortion effect of the additional lens has to be evaluated.

- kinfu update weight experiments


%\section{System plan}
%	\begin{itemize}
%		\item Calibration
%			\begin{itemize}
%				\item Depth image based
%				\item IR image based
%				\item Undistort
%			\end{itemize}
%		\item Low resolution 3D reconstruction
%			\begin{itemize}
%				\item Multi View Stereo (VSFM, Bundler, Photosynth, etc.)
%				\item Kinect Fusion, PCL::Kinfu
%			\end{itemize}
%		\item Pose estimation of new input
%			\begin{itemize}
%				\item VSFM ( for RGB input )
%				\item ICP ( for RGB-D input )
%				\item Gyro+IMU (a noisy graph of acquired data)
%			\end{itemize}
%		\item Mesh subdivision (Linear, Butterfly, Catmull-Clark)
%		\item 2D projection
%			\begin{itemize}
% 				\item Raycasting using Octree
%				\item GLSL projection
%			\end{itemize}
%		\item Yang filtering
%			\begin{itemize}
%				\item Iterative Cross Bilateral filtering
%				\item Subpixel accurracy
%			\end{itemize}
%		\item Mesh enhancement by backprojection
%	\end{itemize}
%	
%\section{System design}
%	\begin{itemize}
%		\item Implementation details and takeaway experience of "System plan" elements
%		\item Merge into previous?
%	\end{itemize}
%	
%\section{Evaluation}
%	\begin{itemize}
% 	\item Experiments
%		\begin{itemize}
%			\item Calibration
%				\begin{itemize}
%					\item Kinect built in calibration
%					\item Bogouet calibration with and without lens distortion (PARAMETERS explained)
%					\item Undistort effectivity (project undistorted depth map to 3D)
%				\end{itemize}
%				
%			\item Filtering
%				\begin{itemize}
%					\item CrossBilateral filter vs. Bilateral
%					\item Yang vs. CrossBilateral (PARAMETERS explained)
%					\item Trilateral, Guided, Pixel Weighted Average Strategy (Garcia et al,ImProc, 2010) - {\bf IF there's time}
%				\end{itemize}
%			\item Kinect fusion related
%			\begin{itemize}
%				\item Kinfu vs. Kinfu w/ filtering turned OFF
%				\item Kinfu voxel grid resolution ($386^3$,$512^3$,$640^3$)
%				\item Kinfu + Yang( original kinect depth frames )
%				\item Kinfu + Yang( "arbitrary pose" )
%			\end{itemize}
%		\end{itemize}
%	\item Existing libraries used
%	\item Data capturing details
%	\item No existing datasets compared (no time)
%
%	\end{itemize}
%
%\section{Results}
%\begin{itemize}
%	\item Upsampling with original depth frames
%	\item Upsampling with simulated "arbitrary pose"
%
%	\item original yang images
%	\item subpixel refinement
%	
%	\item So, Yang is is not suitable for this resolution, it only could improve, when the input was much noisier
%	\item Different approaches can be plugged in to this framework to enhance depth
%\end{itemize}
%\section{Conclusions, Future work}
%	\begin{itemize}
%		\item 3D reconstruction improvements
%			\begin{itemize}
%				\item Improve on smoothing effect of voxel grid weights
%				\item Pose estimation improvements
%			\end{itemize}
%		\item RGB high-resolution capture, vsfm pose estimation 
%		\item Enhancement by best pose selection from input image collection for given pixel/region
%	\end{itemize}
 

\citep{Lowe04}

%\bibliographystyle{plainnat}
\bibliography{litrev}
	
\end{document}


